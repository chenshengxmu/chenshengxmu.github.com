---
layout: post
url: https://www.huxiu.com/article/40272
name: 虎嗅
time: 2014-08-16 11:31
title: 伊恩?勒坤，Facebook“大脑”告诉你深度学习怎么来？
---
注：国外媒体发表文章对Facebook人工智能实验室负责人伊恩?勒坤(Yann LeCun)进行评述，文章谈及勒坤所研究的卷积神经网络对人工智能产生深远影响，潜力不容小觑。此外还介绍了他开发的书写数字识别系统LeNets以及他对反向传播算法的研究成果，并对深度学习的前景进行评析。文章由网易科技编译

马克?扎克伯格精心挑选了深度学习专家伊恩?勒坤担任Facebook人工智能实验室的负责人。该实验室于去年年底成立。作为纽约大学任教已久的教授，伊恩?勒坤对深度学习（deep learning）的研究成绩斐然，在IEEE世界计算智能大会中荣获神经网络先锋奖。深度学习，作为人工智能的一种形式，旨在更密切地模仿人类大脑。最初，大多数人工智能研究人员公开表态对深度学习嗤之以鼻，但短短几年后，它却突然在整个高科技领域蔓延开来，横跨谷歌、微软、百度再至Twitter。

这些高科技公司正在探索深度学习的一种特殊形态——卷积神经网络，旨在打造可以自动理解自然语言以及识别图像的Web服务。谷歌Android手机的语音识别系统就是基于神经网络而开发的。百度利用神经网络对一种新型的可视化搜索引擎进行研发。研究深度学习的学者不在少数，但它获得成功，勒坤功不可没。微软的机器学习专家莱昂?伯托（Leon Bottou）早期曾与勒坤合作。

“对于可视化卷积神经网络，勒坤的付出远甚于他人。”

面临巨大怀疑，勒坤仍然力挺神经网络。要让神经网络正常运作需要功能强大的计算机和庞大的数据集，但上世纪80年代勒坤刚刚接触这一全新领域时，却不具备这些支持条件。当时刚刚步入计算机时代，科学家们对人工智能报以热切的期望，但神经网络受限于那时的条件，无力满足科学家的愿景，因而不被看好。要想在权威学术期刊发表与神经网络相关的文章困难重重。时至90年代乃至21世纪初，这一状况依旧没有得到改善。

但勒坤仍然坚持不懈。终于，如今电脑技术大迈步向前，为深度学习提供了必要的技术支持，其潜力亦得以开发。

在加入Facebook之前的二十多年，勒坤在贝尔实验室中工作，这段时间内，他研发出了一个可以识别手写数字的系统，并称之为LeNet。贝尔实验室作为世界上最著名的计算机研究实验室，是晶体管、Unix操作系统和C语言的发源地。

LeNet能够自动读取银行支票，它标志着卷积神经网络首次被应用于实践中。伯托表示，“卷积网络原本像是个小玩具，勒坤将之应用于规模更广的实际问题中。”

上个世纪70以及80年代，认知机（cognitron）和神经认知机(Neocognitron)这些早期的神经网络模型能够自主学习从数据中识别图形，并且无需人类的过多提示。但这类模型都相当复杂，研究人员无法完全弄清楚如何使它们运行无误。

“当时缺少一种监督学习算法，现在我们称之为反向传播算法（Back propagation）。这种算法能有效地使错误率最小化。”

卷积网络是由相互连通的卷积层组成，与大脑中处理视觉信息的视觉皮层十分类似。卷积网络的不同之处在于，它们可以重复使用一张图像中多个位置的相同过滤器。举例而言，一旦卷积网络学会了在某个位置识别人脸，那么它也可以自动在其他位置识别人脸。这种原理也适用于声波和手写文字。

百度研究院负责人吴恩达（Andrew Ng）认为，这使人工神经网络能够快速接受培训。

“内存占用空间小，不需要对图像中每个位置的过滤器进行单独存储，从而使神经网络非常适合于创建可扩展的深网（deep nets）。这也令卷积神经网络具有善于识别图形的优点。”

当卷积神经网络接收到图像（即输入）时，它将其转换为代表特征的数字阵列，并对每个卷积层中“神经元”进行调整以识别数字中某些图形。低级神经元能够识别基本形状，而高级神经元则能够识别狗或人等更复杂的形态。每个卷积层与相邻的层互通，当信息在网络中传播时，就会得出平均值。最后，网络通过猜测图像中是什么图形从而得出输出结果。

如果网络出错，工程师可以对层与层之间的连接进行微调，以便得到正确答案。而神经网络能够自主进行微调，因而更胜一筹。这时反向传播算法就开始发挥作用了。

反向传播算法的原理是计算误差，并根据误差对卷积层所接收的强度进行更新。上个世界80年代中期，David Rumelhart、Geoffrey Hinton及Ronald Williams提出反向传播算法，即同时为多重输入计算误差，并取平均值。然后通过网络将平均误差从输出层到输入层反向传播。

勒坤对反向传输算法的构想与上述不同，他并未采取平均值，而是为每个样本计算出误差。他的这种方法成效不错，速度更快。

据伯托透露，勒坤得出这一办法，实际上是阴错阳差的结果。当时的电脑不太给力。他们不得不想办法，希望用尽可能少的电脑配置，尽可能快速地计算出误差。这在当时似乎是蒙混过关的做法，但如今却成为人工智能工具箱的重要部分。它就是随机梯度下降算法（stochastic gradient descent）。

勒坤的LeNets已广泛应用于世界各地的自动取款机和银行，用以识别支票上的手写字迹。但仍有人持怀疑态度。勒坤表示，“目前我们所获得的进展还不足以说服计算机视觉领域承认卷积神经网络的价值。”部分原因在于，虽然卷积神经网络功能强大，但没有人知道它为什么这么强大。目前还未能揭开这项技术谜一般的内在原理。

1995年3月的一个下午，瓦普尼克和拉里?杰克尔（Larry Jackel，招募瓦普尼克和勒坤进入贝尔实验室）两人打了个赌。杰克尔认为，到2000年，深度人工神经网络（deep artificial neural nets）的内在原理将明朗化。瓦普尼克则坚持将时限推后至2005年。他们还较真地把赌注内容写在纸上，并在几位见证人面前签了名。勒坤和伯托当时都在场。

打赌双方最终难解胜负。2000年，神经网络的核心原理仍然笼罩在神秘面纱下，哪怕是现在，研究人员也无法用数学方法参透个中奥妙。2005年，深度神经网络在自动取款机和银行中获得广泛应用，虽然人们仍未能掌握核心原理，但勒坤在上个世纪80年代中期和90年代初的研究工作为深度神经网络的解密奠定了重要根基。

“很少有某项技术能在问世20或25年后，虽然基本上未经改变，但在时间的考验下被证实是最优异的。人们接受它的速度是惊人。我过去从未遇见过这样的情况。”

目前使用最广泛的卷积神经网络几乎完全依赖于监督学习（supervised learning）。这意味着，如果想让神经网络学会如何识别某一特定对象，就必须对几个样本进行标注。无监督学习（unsupervised learning）是指从未经标记的数据展开学习，这更接近人脑的学习方式。目前一些深度学习的研究者正在探索这一领域。

“我们对大脑如何学习几近完全陌生。人们已经知道神经元突触能够自我调整，但我们对大脑皮层的机理尚不明确。我们知道最终答案是无监督学习，但却无力解答。”?

反向传播算法不太可能体现出人类大脑的运作机理，所以研究者正在探索其他算法。此外，卷积网络在收集数据或计算平均值时，效果并非十全十美，所以当前研究者也尽力做出改进。辛顿表示，“卷积网络会丢失信息。”

以人脸为例。系统如果学会识别眼睛和嘴唇之类的面部特征，便能有效地识别出图像中有人脸，但无力分辨出不同面孔之间的差异。它也无法很好地找出眼睛在脸上的准确位置。高科技公司和政府想要创建有关用户或居民详尽的数字档案，以上所提及的缺陷将成为无法回避的短板。

勒坤的研究也许不算完美，但当前却是这一领域的尖端理论。

