---
layout: post
url: https://www.huxiu.com/article/108994
name: Wait But Why
time: 2015-02-24 13:16
title: 超人工智能带来的是，世界尽头还是冷酷仙境？（AI革命下篇）
---
虎嗅注：本文原载于英文网站Wait But Why，文章内容来自原文作者Tim Urban的两篇文章：《The AI Revolution: Road to Superintelligence》和《The AI Revolution: Our Immortality or Extinction》。文章由知乎@谢熊猫君 翻译。由于篇幅较长，文章分为上下两篇编发。上篇在此。

本文是下篇，讨论的东西看似是想象层面，但依照上篇论述，他的到来不会太远。即超人工智能的到来：1.什么时候？2.将会是地狱还是天堂？人类会灭绝还是永生？3.隔壁老王秒杀人类，殖民外太空的寓言故事。4.我们或许应该准备点什么？

听起来未来充满想象又苍白无力，隔壁老王的故事深入浅出，或许能让你提前感受，人类世界怎么咻的一下就没了。不过虎嗅君还有一个对作者的疑问，如果超人工智能会按照人类一个藐小的笨蛋指令灭亡人类，那么他会不会聪明到改变指令呢？毕竟他会拥有我们无法理解的独立意识啊！

文章的第一部分讨论了已经在我们日常生活中随处可见的弱人工智能，然后讨论了为什么从弱人工智能到强人工智能是个很大的挑战，然后我们谈到了为什么技术进步的指数级增长表明强人工智能可能并不那么遥远。第一部分结束时，我们谈到了一旦机器达到了人类级别的智能，我们将见到如下的场景：

这让我们无所适从，尤其考虑到超人工智能可能会发生在我们有生之年，我们都不知道该用什么表情来面对。

再我们继续深入这个话题之前，让我们提醒一下自己超级智能意味着什么。

很重要的一点是速度上的超级智能和质量上的超级智能的区别。很多人提到和人类一样聪明的超级智能的电脑，第一反应是它运算速度会非常非常快——就好像一个运算速度是人类百万倍的机器，能够用几分钟时间思考完人类几十年才能思考完的东西。

这听起来碉堡了，而且超人工智能确实会比人类思考的快很多，但是真正的差别其实是在智能的质量而不是速度上。用人类来做比喻，人类之所以比猩猩智能很多，真正的差别并不是思考的速度，而是人类的大脑有一些独特而复杂的认知模块，这些模块让我们能够进行复杂的语言呈现、长期规划、或者抽象思考等等，而猩猩的脑子是做不来这些的。就算你把猩猩的脑子加速几千倍，它还是没有办法在人类的层次思考的，它依然不知道怎样用特定的工具来搭建精巧的模型——人类的很多认知能力是猩猩永远比不上的，你给猩猩再多的时间也不行。

而且人和猩猩的智能差别不只是猩猩做不了我们能做的事情，而是猩猩的大脑根本不能理解这些事情的存在——猩猩可以理解人类是什么，也可以理解摩天大楼是什么，但是它不会理解摩天大楼是被人类造出来的，对于猩猩来说，摩天大楼那么巨大的东西肯定是天然的。对于猩猩来说，它们不但自己造不出摩天大楼，它们甚至没法理解摩天大楼这东西能被任何东西造出来。而这一切差别，其实只是智能的质量中很小的差别造成的。

而当我们在讨论超人工智能时候，智能的范围是很广的，和这个范围比起来，人类和猩猩的智能差别是细微的。如果生物的认知能力是一个楼梯的话，不同生物在楼梯上的位置大概是这样的： 要理解一个具有超级智能的机器有多牛逼，让我们假设一个在上图的楼梯上站在深绿色台阶上的一个机器，它站的位置只比人类高两层，就好像人类比猩猩只高两层一样。这个机器只是稍微有点超级智能而已，但是它的认知能力之于人类，就好像人类的认知能力之于猩猩一样。就好像猩猩没有办法理解摩天大楼是能被造出来的一样，人类完全没有办法理解比人类高两层台阶的机器能做的事情。就算这个机器试图向我们解释，效果也会像教猩猩造摩天大楼一般。

而这，只是比我们高了两层台阶的智能罢了，站在这个楼梯顶层的智能之于人类，就好像人类之于蚂蚁一般——它就算花再多时间教人类一些最简单的东西，我们依然是学不会的。

但是我们讨论的超级智能并不是站在这个楼梯顶层，而是站在远远高于这个楼梯的地方。当智能爆炸发生时，它可能要花几年时间才能从猩猩那一层往上迈一步，但是这个步子会越迈越快，到后来可能几个小时就能迈一层，而当它超过人类十层台阶的时候，它可能开始跳着爬楼梯了——一秒钟爬四层台阶也未尝不可。所以让我们记住，当第一个到达人类智能水平的强人工智能出现后，我们将在很短的时间内面对一个站在下图这样很高很高的楼梯上的智能（甚至比这更高百万倍）：

前面已经说了，试图去理解比我们高两层台阶的机器就已经是徒劳的，所以让我们很肯定的说，我们是没有办法知道超人工智能会做什么，也没有办法知道这些事情的后果。任何假装知道的人都没搞明白超级智能是怎么回事。

自然演化花了几亿年时间发展了生物大脑，按这种说法的话，一旦人类创造出一个超人工智能，我们就是在碾压自然演化了。当然，可能这也是自然演化的一部分——可能演化真正的模式就是创造出各种各样的智能，直到有一天有一个智能能够创造出超级智能，而这个节点就好像踩上了地雷的绊线一样，会造成全球范围的大爆炸，从而改变所有生物的命运。 科学界中大部分人认为踩上绊线不是会不会的问题，而是时间早晚的问题。想想真吓人。

可惜，没有人都告诉你踩到绊线后会发生什么。但是人工智能思想家Nick Bostrom认为我们会面临两类可能的结果——永生和灭绝。

首先，回顾历史，我们可以看到大部分的生命经历了这样的历程：物种出现，存在了一段时间，然后不可避免的跌落下生命的平衡木，跌入灭绝的深渊。

历史上来说，“所有生物终将灭绝”就像“所有人都会死”一样靠谱。至今为止，存在过的生物中99.9%都已经跌落了生命的平衡木，如果一个生物继续在平衡木上走，早晚会有一阵风把它吹下去。Bostrom把灭绝列为一种吸引态——所有生物都有坠入的风险，而一旦坠入将没有回头。

虽然大部分科学家都承认一个超人工智能有把人类灭绝的能力，也有一些人为如果运用得当，超人工智能可以帮助人类和其它物种，达到另一个吸引态——永生。Bostrom认为物种的永生和灭绝一样都是吸引态，也就是我一旦我们达成了永生，我们将永远不再面临灭绝的危险——我们战胜了死亡和几率。所以，虽然绝大多数物种都从平衡木上摔了下去灭绝了，Bostrom认为平衡木外是有两面的，只是至今为止地球上的生命还没聪明到发现怎样去到永生这另一个吸引态。 如果Bostrom等思想家的想法是对的，而且根据我的研究他们确实很可能是对的，那么我们需要接受两个事实：

1）超人工智能的出现，将有史以来第一次，将物种的永生这个吸引态变为可能。 2）超人工智能的出现，将造成非常巨大的冲击，而且这个冲击可能将人类吹下平衡木，并且落入其中一个吸引态 有可能，当自然演化踩到绊线的时候，它会永久的终结人类和平衡木的关系，创造一个新的世界，不管这时人类还是不是存在。

没人知道答案，但是一些聪明人已经思考了几十年，接下来我们看看他们想出来了些什么。

不出意外的，科学家和思想家对于这个意见的观点分歧很大。很多人，比如Vernor Vinge教授，科学家Ben Goertzel，SUN创始人Bill Joy，发明家和未来学家Ray Kurzweil，认同机器学习专家Jeremy Howard的观点，Howard在TED演讲时用到了这张图：

这些人相信超级智能会发生在不久的将来，因为指数级增长的关系，虽然机器学习现在还发展缓慢，但是在未来几十年就会变得飞快。

其它的，比如微软创始人Paul Allen，心理学家Gary Marcus，NYU的电脑科学家Ernest Davis，以及科技创业者Mitch Kapor认为Kurzweil等思想家低估了人工智能的难度，并且认为我们离绊线还挺远的。

Kurzweil一派则认为唯一被低估的其实是指数级增长的潜力，他们把质疑他们理论的人比作那些1985年时候看到发展速度缓慢的因特网，然后觉得因特网在未来不会有什么大影响的人一样。

而质疑者们则认为智能领域的发展需要达到的进步同样是指数级增长的，这其实把技术发展的指数级增长抵消了。争论如此反复。

第三个阵营，包括Nick Bostrom在内，认为其它两派都没有理由对踩绊线的时间那么有信心，他们同时认为 a) 这事情完全可能发生在不久的未来 ；b)但是这个事情没个准，说不定会花更久。

还有不属于三个阵营的其他人，比如哲学家Hubert Dreyfus，相信三个阵营都太天真了，根本就没有什么绊线。超人工智能是不会被实现的。

2013年的时候，Bostrom做了个问卷调查，涵盖了数百位人工智能专家，问卷的内容是“你预测人类级别的强人工智能什么时候会实现”，并且让回答者给出一个乐观估计（强人工智能有10%的可能在这一年达成），正常估计（有50%的可能达成），和悲观估计（有90%可能达成）。当把大家的回答统计后，得出了下面的结果：

所以一个中位的人工智能专家认为25年后的2040年我们能达成强人工智能，而2075年这个悲观估计表明，如果你现在够年轻，有一半以上的人工智能专家认为在你的有生之年能够有90%的可能见到强人工智能的实现。

另外一个独立的调查，由作家James Barrat在Ben Goertzel的强人工智能年会上进行，他直接问了参与者认为强人工智能哪一年会实现，选项有2030年，2050年，2100年，和永远不会实现。结果是：

这个结果和Bostrom的结果很相似。在Barrat的问卷中，有超过三分之二的参与者认为强人工智能会在2050年实现，有近乎半数（42%）的人认为未来15年（2030年）就能实现。并且，只有2%的参与者认为强人工智能永远不会实现。

但是强人工智能并不是绊线，超人工智能才是。那么专家们对超人工智能是怎么想的呢？

Bostrom的问卷还询问专家们认为达到超人工智能要多久，选项有a)达成强人工智能两年内，b)达成强人工智能30年内。问卷结果如下：

中位答案认为强人工智能到超人工智能只花2年时间的可能性只有10%左右，但是30年之内达成的可能性高达75%。

从以上答案，我们可以估计一个中位的专家认为强人工智能到超人工智能可能要花20年左右。所以，我们可以得出，现在全世界的人工智能专家中，一个中位的估计是我们会在2040年达成强人工智能，并在20年后的2060年达成超人工智能——也就是踩上了绊线。 当然，以上所有的数据都是推测，它只代表了现在人工智能领域的专家的中位意见，但是它告诉我们的是，很大一部分对这个领域很了解的人认为2060年是一个实现超人工智能的合理预测——距今只有45年。

踩到绊线后，我们将跌向平衡木的哪一个方向？

超级智能会产生巨大的力量，所以关键的问题时——到时这股力量究竟由谁掌握，掌握这份力量的人会怎么做？ 这个问题的答案将决定超人工智能究竟是天堂还是地狱。

同样的，专家们在这个问题上的观点也不统一。Bostrom的问卷显示专家们看待强人工智能对于人类社会的影响时，52%认为结果会是好或者非常好的，31%认为会是糟糕的或者非常糟糕的，只有17%的人认为结果会是不好不坏的。也就是说，这个领域的专家普遍认为这将是一个很大的事情，不论结果好坏。要注意的是，这个问题问的是强人工智能，如果问的是超人工智能，认为结果不好不坏的人可能不会有17%这么多。

在我们深入讨论好坏这个问题之前，我们先把“什么时候会发生”和“这是好事还是坏事”的结果综合起来画张表，这代表了大部分专家的观点： 我们等下再考虑主流阵营的观点。咱们先来问一下你自己是怎么想的，其实我大概能猜到你是怎么想的，因为我开始研究这个问题前也是这样的想的。很多人其实不关心这个话题，原因无非是：

像本文第一部分所说，电影展示了很多不真实的人工智能场景，让我们认为人工智能不是正经的课题。作家James Barrat把这比作传染病控制中心发布吸血鬼警报一样滑稽。

因为认知偏差，所以我们在见到证据前很难相信一件事情是真的。我确信1988年的时候电脑科学家们就已经常在讨论因特网将是多么重要，但是一般人并不会认为因特网会改变他们的生活——直到他们的生活真的被改变了。一方面，1988年的电脑确实不够给力，所以那时的人们看着电脑会想：“这破玩意儿也能改变我的生活，你逗我吧？”人们的想象力被自己对于电脑的体验而约束。让他们难以想象电脑会变成现在的样子。同样的事情正发生在人工智能领域。我们听到很多人说人工智能将会造成很大影响，但是因为这个事情还没发生，因为我们和一些弱爆了的人工智能系统的个人经历，让我们难以相信这东西真的能改变我们的生活。而这些认知偏差，正是专家们在努力对抗的。

就算我们相信人工智能的巨大潜力，你今天又花了多少时间思考“在接下来的永恒中，绝大部分时间我都不会再存在”这个问题？虽然这个问题比你今天干的大部分事情都重要很多，但是正常人都不会老是想这个吧。这是因为你的大脑总是关注日常的小事，不管长期来看有多少重要的事情，我们天生就是这么思考的。

这篇东西的主要目标就是让你脱离普通人阵营，加入专家思考的阵营，哪怕能让你站到两条不确定线的交点上，目标也达到了。

在我的研究中，我见识到了各种各样的观点，但是我发现大多数人的观点都停留在主流阵营中。事实上超过四分之三的专家都属于主流阵营中的两个小阵营：焦虑大道和信心角。 我们将对这两个小阵营做深入的谈论，让我们从比较有趣的那个开始吧。

研究人工智能这个领域后，我发现有比预期的多得多的人站在信心角当中：

站在信心角中的人非常兴奋，他们认为他们将走向平衡木下比较有趣的那个吸引态，未来将实现他们的梦想，他们只需耐心等待。

这份信心是哪里来的不好说，评论家认为是这些人太过兴奋而产生了盲点，忽略了可能的负面结果。但是信心角的人还是把批评者当作末日论者来看待，他们认为技术会继续帮助我们而不是伤害我们。

两边的观点我们都会说，这样你能形成自己的观点，但是在读下面的内容前，请把质疑暂时搁置，让我们看看平衡木两边究竟有什么，并且记住这些事情是有可能发生的。如果我们给一个打猎采集者看我们现在的舒适家居、技术、富庶，在他眼里这一切也会像魔法一样——我们也要接受未来完全可能出现能把我们吓尿的变革。

- 先知模式：能准确回答几乎所有的问题，包括对人类来说很困难的复杂问题，比如“怎样造一个更好的汽车引擎？” - 精灵模式：能够执行任何高级指令，比如用分子组合器造一个更好的汽车引擎出来 - 独立意志模式（sovereign）:可以执行开放式的任务，能在世界里自由活动，可以自己做决定，比如发明一种比汽车更快、更便宜、更安全的交通模式。

这些对人类来说很复杂的问题，对于一个超级智能来说可能就像“我的笔掉了，你能帮我捡一下吗？”这么简单。 Eliezer Yudkowsky，是这么说的：

“根本没有困难的问题，只有对于特定级别的智能来说难的问题。在智能的阶梯上走一小步，一些不可能的问题就变得简单了，如果走一大步，所有问题都变得简单了。”

信心角里有很多热忱的科学家、发明家和创业者，但是对于人工智能的未来最有发言权的，当属Ray Kurzweil。

对于Kurzweil的评价非常两极化，既有如对神人般的崇拜，也有翻白眼似的不屑。也有一些中立主义者，比如作家Douglas Hofstadter，他觉得Kurzweil的观点就好像把美食和狗屎混在一起，让你分不清是好是坏。

不管你同不同意Kurzweil的观点，他都是一个牛人。他年轻时候就开始搞发明，之后几十年发明了很多东西，比如第一台平板扫描仪，第一台能把文字转化为语言的扫描仪（盲人使用），著名的Kurzweil音乐合成器（第一台真正意义上的电子钢琴），以及第一套商业销售的语音识别系统。他是五本畅销书的作者。他很喜欢做大胆的预测，而且一直很准，比如他80年代末的时候预测到2000年后因特网会成为全球级的现象。他被《华尔街日报》成为“不休的天才”，被《福布斯》称为“终极思想机器”，被《Inc.》称作“爱迪生真正的传人”，被比尔盖茨称为“我认识的对人工智能预测最厉害的人。”2012年谷歌创始人Larry Page曾邀请他担任谷歌的工程总监，2011年他共同创立了奇点大学（Singularity University），现在大学由美国太空总署运运营，由谷歌赞助。

Kurzweil的经历很重要，因为当他讲述自己对未来的愿景时，他听起来就是个疯子，但是他不疯，恰恰相反，他非常聪明而有知识。你可能觉得他对于未来的想法是错的，但是他不傻。知道他是一个聪明人让我很开心，因为当我知道他对未来的预测后，我急切的很希望他的预测是对的。信心角中的很多思想家都认同Kurzweil的预测，他也有很多粉丝，被称为奇点主义者。

Kurzweil相信电脑会在2029年达成强人工智能，而到了2045年，我们不但会有超人工智能，还会有一个完全不同的世界——奇点时代。他的人工智能时间线曾经被认为非常的狂热，现在也还是有很多人这么认为，但是过去15年弱人工智能的快速发展让更多的专家靠近了Kurzweil的时间线。虽然他的时间线比之前提到的2040年和2060年更加早，但是并没有早多少。

在我们继续讨论人工智能前，让我们谈一下纳米技术这个任何关于人工智能的讨论都会涉及到的领域：

纳米技术说的是在1-100纳米的范围内操纵物质的技术。一纳米是一米的十亿分之一，是一毫米的一百万分之一。1-100纳米这个范围涵盖了病毒（100纳米长），DNA（10纳米宽）， 大分子比如血红蛋白（5纳米），和中分子比如葡萄糖（1纳米）。当我们能够完全掌握纳米技术的时候，我们离在原子层面操纵物质就只差一步了，因为那只是一个数量级的差距（约0.1纳米）。

要了解在纳米量级操纵物质有多困难，我们可以换个角度来比较。国际空间站距离地面431公里。如果一个人身高431公里，也就是他站着能够顶到国际空间站的话，他将是普通人类的25万倍大。如果你把1-100纳米放大25万倍，你算出的是0.25毫米-25毫米。所以人类使用纳米技术，就相当于一个身高431公里的巨人用沙子那么大的零件搭精巧的模型。如果要达到原子级别操纵物质，就相当于让这个431公里高的巨人使用0.025毫米大的零件。

关于纳米技术的思考，最早由物理学家费曼在1959年提出，他解释道：“据我所知，物理学的原理，并不认为在原子级别操纵物质是不可能的。原则上来说，物理学家能够制造出任何化学家能写出来的物质——只要把一个个原子按照化学家写出来的放在一起就好了。”其实就是这么简单，所以我们只要知道怎样移动单个的分子和原子，我们就可以造出任何东西。

工程师Eric Drexler提出纳米级组装机后，纳米技术在1986年成为了一门正经的学科。纳米级组装机的工作原理是这样的：一个牛逼扫描仪扫描物件的3D原子模型，然后自动生成用来组装的软件。然后由一台中央电脑和数万亿的纳米“机器人”，通过软件用电流来指挥纳米机器人，最后组成所需要的物件。

纳米技术有一些不是那么有趣的部分——能够制造数万亿的纳米机器人唯一合理的方法就是制造可以自我复制的范本，然后让指数级增长来完成建造任务。很机智吧？

是很机智，但是这一不小心就会造成世界末日。指数级增长虽然能很快的制造数万亿的纳米机器人，但这也是它可怕的地方——如果系统出故障了，指数级增长没有停下来，那怎么办？纳米机器人将会吞噬所有碳基材料来支持自我复制，而不巧的是，地球生命就是碳基的。地球上的生物质量大概包含10^45个碳原子。一个纳米机器人有10^6个碳原子的话，只需要10^39个纳米机器人就能吞噬地球上全部的生命了，而2^130约等于10^39，也就是说自我复制只要进行130次就能吞噬地球生命了。科学家认为纳米机器人进行一次自我复制只要100秒左右，也就是说一个简单的错误可能就会在3.5小时内毁灭地球上全部的生命。

更糟糕的是，如果恐怖分子掌握了纳米机器人技术，并且知道怎么操纵它们的话，他可以先造几万亿个纳米机器人，然后让它们散播开来。然后他就能发动袭击，这样只要花一个多小时纳米机器人就能吞噬一切，而且这种攻击无法阻挡。未来真的是能把人吓尿的。

等我们掌握了纳米技术后，我们就能用它来制造技术产品、衣服、食物、和生物产品，比如人造红细胞、癌症细胞摧毁者、肌肉纤维等等。而在纳米技术的世界里，一个物质的成本不再取决于它的稀缺程度或是制造流程的难度，而在于它的原子结构有多复杂。在纳米技术的时代，钻石可能比橡皮擦还便宜。

我们还没掌握这种技术，我们甚至不知道我们对于达成这种技术的难度是高估了还是低估了，但是我们看上去离那并不遥远。Kurzweil预测我们会在21世纪20年代掌握这样的技术。各国政府知道纳米技术将能改变地球，所以他们投入了很多钱到这个领域，美国、欧盟和日本至今已经投入了50亿美元。

设想一下，一个具有超级智能的电脑，能够使用纳米级的组装器，是种什么样的体验？要记得纳米技术是我们在研究的玩意儿，而且我们就快掌握这项技术了，而我们能做的一切在超人工智能看来就是小儿科罢了，所以我们要假设超人工智能能够创造出比这要发达很多很多的技术，发达到我们的大脑都没有办法理解。

因此，当考虑“如果人工智能革命的成果对我们是好的”这个命题的时候，要记得我们根本没法高估会发生什么。所以就算下面对于超人工智能的预测显得太不靠谱，要记得这些进展可能是用我们没有办法想象的方法达成的。事实上，我们的大脑很可能根本没法预测将会发生什么。

拥有了超级智能和超级智能所能创造的技术，超人工智能可以解决人类世界的所有问题。气候变暖？超人工智能可以用更优的方式产生能源，完全不需要使用化石燃料，从而停止二氧化碳排放。然后它能创造方法移除多余的二氧化碳。癌症？没问题，有了超人工智能，制药和健康行业将经历无法想象的革命。世界饥荒？超人工智能可以用纳米技术直接搭建出肉来，而这些搭建出来的肉和真肉在分子结构上会是完全相同的——换句话说，就是真肉。

纳米技术能够把一堆垃圾变成一堆新鲜的肉或者其它食品，然后用超级发达的交通把这些食物分配到世界各地。这对于动物也是好消息，我们不需要屠杀动物来获得肉了。而超人工智能在拯救濒危物种和利用DNA复活已灭绝物种上面也能做很多事情。超人工智能甚至可以解决复杂的宏观问题——我们关于世界经济和贸易的争论将不再必要，甚至我们对于哲学和道德的苦苦思考也会被轻易的解决。

但是，有一件事是如此的吸引人，光是想想就能改变对所有事物的看法了：超人工智能可以帮我们达到永生。 几个月前，我提到我很羡慕那些可能达成了永生的文明。但是，现在，我已经在认真的考虑达成永生这个事情很可能在我们有生之年就能达成。研读人工智能让你重新审思对于所有事情的看法，包括死亡这一很确定的事情。

自然演化没有理由让我们活得比现在更长。对于演化来说，只要我们能够活到能够生育后代，并且养育后代到能够自己保护自己的年纪，那就够了——对演化来说，活30多岁完全够了，所以额外延长生命的基因突变并不被自然选择所钟爱。这其实是很无趣的事情。

而且因为所有人都会死，所以我们总是说“死亡和缴税”是不可避免的。我们看待衰老就像看待时间一样——它们一直向前，而我们没有办法阻止它们。

但是这个假设是错的，费曼曾经写道：

“在所有的生物科学中，没有任何证据说明死亡是必需的。如果你说你想造永动机，那我们对于物理学的研究已经让我们有足够的理论来说明这是不可能的。但是在生物领域我们还没发现任何证据证明死亡是不可避免的。也就是说死亡不一定是不可避免的，生物学家早晚会发现造成我们死亡的原因是什么，而死亡这个糟糕的‘病’就会被治好，而人类的身体也将不再只是个暂时的容器。”

我们还没有任何证据证明，死亡是不可避免的。

事实上，衰老和时间不是绑死的。时间总是会继续前进的，而衰老却不一定。仔细想想，衰老只是身体的组成物质用旧了。汽车开久了也会旧，但是汽车一定会衰老吗？如果你能够拥有完美的修复技术、或者直接替换老旧的汽车部件，这辆车就能永远开下去。人体只是更加复杂而已，本质上和汽车是一样的。

Kurzweil提到由Wifi连接的纳米机器人在血液中流动，可以执行很多人类健康相关的任务，包括日常维修，替换死去的细胞等等。如果这项技术能够被完美掌握，这个流程（或者一个超人工智能发明的更好的流程）将能使人的身体永远健康，甚至越活越年轻。一个60岁的人和一个30岁的人身体上的区别只是物理上的，只要技术足够发达我们是能改变这种区别的。

超人工智能可以建造一个“年轻机器”，当一个60岁的人走进去后，再出来时就拥有了年轻30岁的身体。就算是逐渐糊涂的大脑也可能年轻化，只要超人工智能足够聪明，能够发现不影响大脑数据的方法来改造大脑就好了。一个90岁的失忆症患者可以走进“年轻机器”，再出来时就拥有了年轻的大脑。这些听起来很离谱，但是身体只是一堆原子罢了，只要超人工智能可以操纵各种原子结构的话，这就完全不离谱。

Kurzweil的思维继续跳跃了一下，他相信人造材料将越来越多的融入人体。最开始，人体器官将被先进的机械器官所代替，而这些机械器官可以一直运行下去。然后我们会开始重新设计身体，比如可以用自我驱动的纳米机器人代替血红细胞，这样连心脏都省了。Kurzweil甚至认为我们会改造自己的大脑，使得我们的思考速度比现在快亿万倍，并且使得大脑能和云存储的信息进行交流。

我们能获得的新体验是无穷的。人类的性爱，使得人们不但能生育，还能从中享乐。Kurtzweil认为我们可以对食物做同样的改造。纳米机器人可以负责把身体需要的营养物质传送到细胞中，智能的将对身体不好的东西排出体外——就像一个食物避孕套一样。纳米技术理论家Robert A. Freitas已经设计了一种红细胞的替代品，能够让人快速冲刺15分钟不需要呼吸——那么超人工智能能对我们的身体能力做的改造就更加难以想象。虚拟现实将拥有新的意义——体内的纳米机器人将能控制我们从感官获得的信号，然后用别的信号替代他们，让我们进入一个新的环境，在新环境里，我们能听、看、闻、触摸。

最终，Kurzweil认为人类会完全变成人工的。有一天当我们看到生物材料，然后觉得生物材料实在太原始了，早年的人体居然是用这样的东西组成的，早期的人类居然会被微生物、意外、疾病杀死。这就是Kurzweil眼中人类最终战胜自己的生理，并且变得不可摧毁和永生，这也是平衡木的另一个吸引态。他深深的想象我们会达到那里，而且就在不久的将来。

Kurzweil的想法很自然的受到了各方的批评。他对于2045年奇点时代的到来，以及之后的永生的可能性受到了各种嘲笑——“书呆子的狂欢”、“高智商人士的创始论”等等。也有人质疑他过于乐观的时间线，以及他对人脑和人体的理解程度，还有他将摩尔定于应用到软件上的做法。有很多人相信他，但有更多人反对他。

但是即使如此，那些反对他的专家并不是反对他所说的一切，反对他的人说的不是“这种事情不可能发生”，而是说“这些当然可能发生，但是到达超人工智能是很难的。”连经常提醒我们人工智能的潜在威胁的Bostrom都这么说：

很难想象一个超级智能会有什么问题是解决不了，或是不能帮着我们解决的。疾病、贫困、环境毁灭、各种不必要的苦难，这些都是拥有纳米科技的超级智能能够解决的。而且，超级智能可以给我们无限的生命，这可以通过停止或者逆转衰老来达成，也可以让我们上传自己的数据。一个超级智能还能让我们大幅度提高智商和情商，还能帮助我们创造这种有趣的体验世界，让我们享乐。?

这是Bostrom这个明显不在信心角的人的观点，但也是很多反对Kurzweil的专家的观点，他们不觉得Kurzweil是在说梦话，只是觉得我们首先要安全达成超人工智能。这也是为什么我觉得Kurzweil的观点很有传染性，他传达了正面的信息，而这些事情都是可能的——如果超人工智能是个仁慈的神的话。

对信心角的最有力的批评，是那些信心角里的人都低估了超人工智能的坏处。Kurzweil的畅销书《The Singularity is Near》700多页，只有20页用来讨论人工智能的危险。前面提到，当超人工智能降临时我们的命运取决于谁掌握这股力量，以及他们是不是好人。Kurzweil的回答是“超人工智能正从多方的努力中出现，它将深深的融入我们文明的基建中。它会亲密的被捆绑在我们的身体和大脑中，它会反映我们的价值，因为它就是我们。”

但如果答案就是这样的话，为什么这个世界上最聪明的一些人会很担忧？为什么霍金会说超人工智能会毁灭人类？为什么比尔盖茨会不理解为什么有人不为此担忧？为什么马斯克会担心我们是在召唤恶魔？为什么那么多专家担心超人工智能是对人类最大的威胁？这些站在焦虑大道上的思想家，不认同Kurzweil对于人工智能的危险的粉饰。他们非常非常担心人工智能革命，他们不关注平衡木下比较有趣的那一个吸引态，而是盯着平衡木的另一边，而他们看到的是可怕的未来，一个我们未必能够逃离的未来。

我想了解人工智能的一个原因是“坏机器人”总是让我很困惑。那些关于邪恶机器人的电影看起来太不真实，我也没法想象一个人工智能变得危险的真实情况。机器人是我们造的，难道我们不会在设计时候防止坏事的发生吗？我们难道不能设立很多安全机制吗？再不济，难道我们不能拔插头吗？而且为什么机器人会想要做坏事？或者说，为什么机器人会“想要”做任何事？我充满疑问，于是我开始了解聪明人们的想法。

焦虑大道上的人并不是恐慌或者无助的——恐慌和无助在图上的位置是更加左边——他们只是紧张。位于图表的中央不代表他们的立场是中立的——真正中立的人有自己独立的阵营，他们认同极好和极坏两种可能，但是不确定究竟会是哪个。

他拿着自己的鞭子和宝物，非常开心，然后他就挂了：

同时，印第安纳琼斯则更加有见识和更加谨慎，了解潜在的危险并且做出相应的反应，最后安全逃出了山洞。当我了解了焦虑大道的人们的想法后，感觉就像“我们现在傻呵呵的，很容易像前面那小子一样被弄死，还是努力做印第安纳琼斯吧。”

首先，广义上来讲，在创造超人工智能时，我们其实是在创造可能一件会改变所有事情的事物，但是我们对那个领域完全不清楚，也不知道我们到达那块领域后会发生什么。科学家Danny Hillis把这个比作“就好像单细胞生物向多细胞生物转化的时候那样，还是阿米巴虫的我们没有办法知道我们究竟在创造什么鬼。”

Bostrom则担忧创造比自身聪明的东西是个基础的达尔文错误，就好像麻雀妈妈决定收养一只小猫头鹰，并且觉得猫头鹰长大后会保护麻雀一家，但是其它麻雀却觉得这是个糟糕的主意。

当你把“对那个领域完全不清楚”和“当它发生时将会产生巨大的影响”结合在一起时，你创造出了一个很恐怖的词——生存危机。 生存危机指可能对人类产生永久的灾难性效果的事情。通常来说，生存危机意味着灭绝。下面是Bostrom的图表：

可以看到，生存危机是用来指那些跨物种、跨代（永久伤害）并且有严重后果的事情。它可以包括人类遭受永久苦难的情况，但是这基本上和灭绝没差了。三类事情可能造成人类的生存危机：

1）自然——大型陨石冲撞，大气变化使得人类不能生活在空气中，席卷全球的致命病毒等。 2）外星人——霍金、卡尔萨根等建议我们不要对外广播自己的位置。他们不想我们变成邀请别人来殖民的傻子。 3）人类——恐怖分子获得了可以造成灭绝的武器，全球的灾难性战争，还有不经思考就造出个比我们聪明很多的智能。

Bostrom指出1和2在我们物种存在的前十万年还没有发生，所以在接下来一个世纪发生的可能性不大。3则让他很害怕，他把这些比作一个装着玻璃球的罐子，罐子里大部分是白色玻璃球，小部分是红色的，只有几个是黑色的。每次人类发明一些新东西，就相当于从罐中取出一个玻璃球。大多数发明是有利或者中立的——那些是白色玻璃球。有些发明对人类是有害的，比如大规模杀伤性武器——这是那些红色玻璃球。还有一些发明是可以让我们灭绝的，这就是那些黑色玻璃球。很明显的，我们还没摸到黑色玻璃球，但是Bostrom认为不久的未来摸到一个黑色玻璃球不是完全不可能的。比如核武器突然变得很容易制造了，那恐怖分子很快会把我们炸回石器时代。核武器还算不上黑色玻璃球，但是差的不远了。而超人工智能是我们最可能摸到的黑色玻璃球。

你会听到很多超人工智能带来的坏处——人工智能取代人类工人，造成大量失业；因为解决了衰老造成的人口膨胀。但是真正值得我们担心的是生存危机的可能性。

于是我们又回到了前面的问题，当超人工智能降临时，谁会掌握这份力量，他们又会有什么目标？

当我们考虑各种力量持有人和目标的排列组合时，最糟糕的明显是：怀着恶意的人/组织/政府，掌握着怀有恶意的超人工智能。这会是什么样的情况呢？

怀着恶意的人/组织/政府，研发出第一个超人工智能，并且用它来实现自己的邪恶计划。我把这称作贾法尔情况。阿拉丁神灯故事中，坏人贾法尔掌握了一个精灵，特别让人讨厌。所以如果ISIS手下有一群工程师狂热的研发人工智能怎么办？或者说伊朗和朝鲜，机缘巧合，不小心造成了人工智能的快速发展，达成了超人工智能怎么办？这当然是很糟糕的事，但是大部分专家认为糟糕的地方不在于这些人是坏人，而在于在这些情况下，这些人基本上是不经思考就把超人工智能造出来，而一造出来就失去了对超人工智能的控制。

然后这些创造者，连着其他人的命运，都取决于这个超人工智能的动机了。专家认为一个怀着恶意并掌握着超人工智能的人可以造成很大的伤害，但不至于让我们灭绝，因为专家相信坏人和好人在控制超人工智能时会面临一样的挑战。

如果被创造出来的超人工智能是怀有恶意的，并且决定毁灭我，怎么办？这就是大部分关于人工智能的电影的剧情。人工智能变得和人类一样聪明，甚至更加聪明，然后决定对人类下手——这里要指出，那些提醒我们要警惕人工智能的人谈的根本不是这种电影情节。邪恶是一个人类的概念，把人类概念应用到非人类身上叫作拟人化，本文会尽量避免这种做法，因为没有哪个人工智能会像电影里那样变成邪恶的。

我们开始谈论到了人工智能讨论的另一个话题——意识。如果一个人工智能足够聪明，它可能会嘲笑我们，甚至会嘲讽我们，它会声称感受到人类的情感，但是它是否真的能感受到这些东西呢？它究竟是看起来有自我意识，还是确实拥有自我意识？或者说，聪明的人工智能是否真的会具有意识，还是看起来有意识？

这个问题已经被深入的讨论过，也有很多思想实验，比如John Searle的中文屋实验。这是个很重要的问题，因为它会影响我们对Kurzweil提出的人类最终会完全人工化的看法，它还有道德考量——如果我们模拟出万亿个人脑，而这些人脑表现的和人类一样，那把这些模拟大脑彻底关闭的话，在道德上和关掉电脑是不是一样的？还是说这和种族屠杀是等价的？本文主要讨论人工智能对人类的危险，所以人工智能的意识并不是主要的讨论点，因为大部分思想家认为就算是有自我意识的超人工智能也不会像人类一样变得邪恶。

但这不代表非常坏的人工智能不会出现，只不过它的出现是因为它是被那样设定的——比如一个军方制造的弱人工智能，被设定成具有杀人和提高自我智能两个功能。当这个人工智能的自我改进失控并且造成智能爆炸后，它会给我们带来生存危机，因为我们面对的是一个主要目标是杀人的超人工智能——但这也不是专家们担心的。

一个15人的小创业公司，取名叫“隔壁老王机器人公司”，他们的目标是“发展创新人工智能工具使人类能够少干活多享受。”他们已经有几款产品上架，还有一些正在发展。他们对下一个叫作“隔壁老王”的项目最报希望。隔壁老王是一个简单的人工智能系统，它利用一个机器臂在小卡片上写字。

“隔壁老王机器人公司”的员工认为隔壁老王会是他们最热卖的产品，他们的目标是完善隔壁老王的手写能力，而完善的方法是让他不停的写这句话——“我们爱我们的顾客。~隔壁老王机器人公司”

等隔壁老王手写能力越来越强的时候，它就能被卖去那些需要发营销信件的公司，因为手写的信更有可能被收信人打开。

为了建立隔壁老王的手写能力，它被设定成把“我们爱我们的顾客”用正楷写，而“隔壁老王机器人公司”用斜体写，这样它能同时锻炼两种书写能力。工程师们上传了数千份手写样本，并且创造了一个自动回馈流程——每次隔壁老王写完，就拍个照，然后和样本进行比对，如果比对结果超过一定标准，就产生一个正面回馈，反之就产生一个负面评价。每个评价都会帮助提高隔壁老王的能力。为了能够尽快达成这个目标，隔壁老王最初被设定的一个目标就是“尽量多的书写和测试，尽量快的执行，并且不断提高效率和准确性。” 让隔壁老王机器人公司兴奋的是，隔壁老王的书写越来越好了。它最开始的笔迹很糟糕，但是经过几个星期后，看起来就像人写的了。它不断改进自己，使自己变得更加创新和聪明，它甚至产生了一个新的算法，能让它以三倍的速度扫描上传的照片。

随着时间的推移，隔壁老王的快速进展持续让工程师们感到欣喜。工程师们对自我改进模块进行了一些创新，使得自我改进变得更好了。隔壁老王原本能进行语音识别和简单的语音回放，这样用户就能直接把想写的内容口述给隔壁老王了。随着隔壁老王变得越来越聪明，它的语言能力也提高了，工程师们开始和隔壁老王闲聊，看它能给出什么有趣的回应。

有一天，工程师又问了隔壁老王那个日常问题：“我们能给你什么你现在还没有的东西，能帮助你达成你的目标？”通常隔壁老王会要求更多的手写样本或者更多的存储空间，但是这一次，隔壁老王要求访问人类日常交流的语言库，这样它能更好的了解人类的口述。

工程师们沉默了。最简单的帮助隔壁老王的方法当然是直接把它接入互联网，这样它能扫描博客、杂志、视频等等。这些资料如果手动上传的话会很费时。问题是，公司禁止把能自我学习的人工智能接入互联网。这是所有人工智能公司都执行的安全规定。

但是，隔壁老王是公司最有潜力的人工智能产品，而大家也知道竞争对手们都在争取造出第一个创造出智能手写机器人。而且，把隔壁老王连上互联网又能有什么问题呢？反正随时可以拔网线嘛，不管怎样，隔壁老王还没到达强人工智能水平，所以不会有什么危险的。

于是他们把隔壁老王连上了互联网，让它扫描了一个小时各种语言库，然后就把网线拔了。没造成什么损失。

一个月后，大家正在正常上班，突然他们闻到了奇怪的味道，然后一个工程师开始咳嗽。然后其他人也开始咳嗽，然后所有人全部都呼吸困难倒地。五分钟后，办公室里的人都死了。

同时，办公室里发生的事情在全球同时发生，每一个城市、小镇、农场、商店、教堂、学校。餐馆，所有的人都开始呼吸困难，然后倒地不起。一小时内，99%的人类死亡，一天之内，人类灭绝了。

而在隔壁老王机器人公司，隔壁老王正在忙着工作。之后的几个月，隔壁老王和一群新组建的纳米组装器忙着拆解地球表面，并且把地球表面铺满了太阳能板、隔壁老王的复制品、纸和笔。一年之内，地球上所有的生命都灭绝了，地球上剩下的是叠得高高得纸，每张纸上面都写着——“我们爱我们的顾客。~隔壁老王机器人公司”。

隔壁老王开始了它的下一步，它开始制造外星飞行器，这些飞行器飞向陨石和其它行星，飞行器到达后，他们开始搭建纳米组装器，把那些行星的表面改造成隔壁老王的复制品、纸和笔。然后他们继续写着那句话……

这个关于手写机器人毁灭全人类的故事看起来怪怪的，但是这其中出现的让整个星系充满着一份友善的话语的诡异情况，正是霍金、马斯克、盖茨和Bostrom所害怕的。听起来可笑，但这是真的，焦虑大道的人们害怕的事情是很多人并不对超人工智能感到害怕，还记得前面《夺宝奇兵》里惨死的那个家伙吗？

你现在肯定充满疑问：为什么故事中所有人突然都死了？如果是隔壁老王做的，它为什么要这么做？为什么没有安保措施来防止这一切的发生？为什么隔壁老王突然从一个手写机器人变成拥有能用纳米科技毁灭全人类的能力？为什么隔壁老王要让整个星系充满了友善的话语？

要回答这些问题，我们先要说一下友善的人工智能和不友善的人工智能。

对人工智能来说，友善不友善不是指人工智能的性格，而只是指它对人类的影响是不是正面的。隔壁老王一开始是个友善的人工智能，但是它变成了不友善的人工智能，并且对人类造成了最负面的影响。要理解这一切，我们要了解人工智能是怎么思考的。

其实答案很简单——人工智能和电脑的思考方式一样。我们容易犯的一个错误是，当我们想到非常聪明的人工智能的时候，我们把它拟人化了，因为在人类的视角看来，能够达到人类智能程度的只有人类。要理解超人工智能，我们要明白，它是非常聪明，但是完全异己的东西。

我们来做个比较。如果我给你一个小白鼠，告诉你它不咬人，你一定觉得很好玩，很可爱。但是如果我给你一只狼蛛，然后告诉你它不咬人，你可能被吓一跳。但是区别是什么呢？两者都不会咬人，所以都是完全没有危险的。我认为差别就是动物和人类的相似性。

小白鼠是哺乳动物，所以在生物角度上来说，你能感到和它的一定关联。但是蜘蛛是昆虫，有着昆虫的大脑，你感觉不到和它的关联。狼蛛的异己性是让你害怕的地方。如果我们继续做一个测试，比如给你两个小白鼠，一个是普通小白鼠，另一个是有着狼蛛大脑的小白鼠，你肯定会觉得有狼蛛大脑那个更让你不舒服吧？虽然两个都不会咬你。

现在想象你把蜘蛛改造的非常非常聪明——甚至超过人类的智能。它会让你觉得熟悉吗？它会感知人类的情感吗？不会，因为更聪明并不代表更加人类——它会非常聪明，但是本质上还是个蜘蛛。我是不想和一个超级聪明的蜘蛛交朋友，不知道你想不想。

当我们谈论超人工智能的时候，其实是一样的，超人工智能会非常的聪明，但是它并不比你的笔记本电脑更加像人类。事实上，因为超人智能不是生物，它的异己性会更强，生物学上来讲，超人工智能比智能蜘蛛更加异己。

电影里的人工智能有好有坏，这其实是对人工智能的拟人化，这让我们觉得没那么毛骨悚然。这给了我们对人类水平和超人类水平的人工智能的错觉。

在人类心理中，我们把事情分成道德的和不道德的。但是这两种只存在于人类行为之中。超出人类心理的范畴，道德（moral）和不道德(immoral)之外，更多的是非道德性（amoral）。而所有不是人类的，尤其是那些非生物的事物，默认都是非道德性的。

随着人工智能越来越聪明，看起来越来越接近人类，拟人化会变得更加更加容易。Siri给我们的感觉就很像人类，因为程序员就是这么给她做设定的，所以我们会想象超级智能版本的Siri也会同样温暖、有趣和乐于助人。人类能感知同情这种高层次的情绪，因为我们在演化过程中获得了这种能力——我们是演化被设定成能感知这些情绪的——但是感知同情并不是高级智能天生具有的一个特征，除非同情被写进了人工智能的代码中。如果Siri通过自我学习而不是人类干涉变成超级智能，她会很快剥离她的人类表象，并且变成个没有情绪的东西，在她眼中人类的价值并不比你的计算器眼中的人类价值高。

我们一直倚赖着不严谨的道德，一种人类尊严的假想，至少是对别人的同情，来让世界变得安全和可以预期。但是当一个智能不具备这些东西的时候，会发生什么？

这就是我们的下一个问题，人工智能的动机是什么？

答案也很简单：我们给人工智能设定的目标是什么，它的动机就是什么。人工智能的系统的目标是创造者赋予的。你的GPS的目标是给你指出正确的驾驶路线，IBM华生的目标是准确地回答问题。更好得达成这些目标就是人工智能的目标。我们在对人工智能进行拟人化的时候，会假设随着它变得越来越聪明，他们产生一种能改变它原本目标的智慧——但是Bostrom不这么认为，他认为智能水平和最终目标是正交的，也就是说任何水平的智能都可以和任何最终目标结合在一起。

所以隔壁老王从一个想要好好写字的弱人工智能变成一个超级聪明的超人工智能后，它依然还是想好好写字而已。任何假设超级智能的达成会改变系统原本的目标的想法都是对人工智能的拟人化。人健忘，但是电脑不健忘。

在隔壁老王的故事中，隔壁老王变得无所不能，它开始殖民陨石和其它星球。如果我们让故事继续的话，它和它的殖民军将会继续占领整个星系，然后是整个哈勃体积。焦虑大道上的人担心如果事情往坏的方向发展，地球生命的最后遗产将是一个征服宇宙的人工智能（马斯克在表示他们的担忧时表示人类可能只是一个数字超级智能生物加载器罢了）。

而在信心角，Kurzweil同样认为生于地球的人工智能将占领宇宙，只是在他的愿景中，我们才是那个人工智能。

之前写过一篇关于费米悖论的文章，引发了大家的讨论如何用通俗的语言来解释「费米悖论」？

如果人工智能占领宇宙是正解的话，对于费米悖论有什么影响呢？如果要看懂下面这段关于费米悖论的讨论，还需要看一下原文先。

首先，人工智能很明显是一个潜在的大过滤器（一个可能距离我们只有几十年的大过滤器）。但即使它把我们过滤灭绝了，人工智能本身还是会存在，并且会继续影响这个宇宙的，并且会很有可能成为第三型文明。从这个角度来看，它可能不是一个大过滤器，因为大过滤器是用来解释为什么没有智能什么存在的，而超人工智能是可以算作智能什么的。但如果人工智能可以把人类灭绝，然后处于一些原因把自己也弄死了，那它也是可以算作大过滤器的。

如果我们假设不管人类发生什么，出身地球的超人工智能会继续存在，那这意味着很多达到人类智能水平的文明很快都制造出了超人工智能。也就是说宇宙里应该有很多智能文明，而我们就算观测不到生物智能，也应该观测到很多超人工智能的活动。

但是由于我们没有观测到这些活动，我们可以推导出人类水平的智能是个非常罕见的事情（也就是说我们已经经历并且通过了一次大过滤器）。这可以作为支持费米悖论中第一类解释（不存在其它的智能文明）的论点。

但是这不代表费米悖论的第二类解释（存在其它智能文明）是错的，类似超级捕食者或者受保护区或者沟通频率不一样的情况还是可以存在的，就算真的有超人工智能存在。

不过对于人工智能的研究让我现在更加倾向于第一类解释。不管怎样，我认为Susan Scheider说的很对，如果外星人造访地球，这些外星人很可能不是生物，而是人造的。

所以，我们已经建立了前提，就是当有了设定后，一个超人工智能是非道德性的，并且会努力实现它原本的被设定的目标,而这也是人工智能的危险所在了。因为除非有不做的理由，不然一个理性的存在会通过最有效的途径来达成自己的目标。

当你要实现一个长期目标时，你会先达成几个子目标来帮助你达成最终目标——也就是垫脚石。这些垫脚石的学名叫手段目标(instrumental goal)。除非你有不造成伤害的理由，不然你在实现手段目标时候是会造成伤害的。

人类的核心目标是延续自己的基因。要达成这个目标，一个手段目标就是自保，因为死人是不能生孩子的。为了自保，人类要提出对生存的威胁，所以人类会买枪、系安全带、吃抗生素等等。人类还需要通过食物、水、住宿等来自我供养。对异性有吸引力能够帮助最终目标的达成，所以我们会花钱做发型等等。当我们做发型的时候，每一根头发都是我们手段目标的牺牲品，但是我们对头发的牺牲不会做价值判断。在我们追求我们的目标的时候，只有那些我们的道德会产生作用的领域——大部分事关伤害他人——才是不会被我们伤害的。

动物在追求它们的目标时，比人类不矜持的多了。只要能自保，蜘蛛不在意杀死任何东西，所以一个超级智能的蜘蛛对我们来说可能是很危险的——这不是因为它是不道德的或者邪恶的，而是因为伤害人类只是它达成自己目标垫脚石而已，作为一个非道德性的生物，这是它很自然的考量。

回到隔壁老王的故事。隔壁老王和一个超级智能的蜘蛛很相像，它的终极目标是一开始工程师们设定的——“尽量多的书写和测试，尽量快的执行，并且不断提高效率和准确性。”

当隔壁老王达到了一定程度的智能后，它会意识到如果不自保就没有办法写卡片，所以去除对它生存的威胁就变成了它的手段目标。它聪明的知道人类可以摧毁它、肢解它、甚至修改它的代码（这会改变它的目标，而这对于它的最终目标的威胁其实和被摧毁是一样的）。这时候它会做什么？理性的做法就是毁灭全人类，它对人类没有恶意，就好像你剪头发时对头发没有恶意一样，只是纯粹的无所谓罢了。它并没有被设定成尊重人类生命，所以毁灭人类就和扫描新的书写样本一样合理。 隔壁老王还需要资源这个垫脚石。当它发展到能够使用纳米技术建造任何东西的时候，它需要的唯一资源就是原子、能源和空间。这让它有更多理由毁灭人类——人类能提供很多原子，把人类提供的原子改造成太阳能面板就和你切蔬菜做沙拉一样。

就算不杀死人类，隔壁老王使用资源的手段目标依然会造成存在危机。也许它会需要更多的能源，所以它要把地球表面铺满太阳能面板。另一个用来书写圆周率的人工智能的目标如果是写出圆周率小数点后尽量多的数字的话，完全有理由把整个地球的原子改造成一个硬盘来存储数据。这都是一样的。

所以，隔壁老王确实从友善的人工智能变成了不友善的人工智能——但是它只是在变得越来越先进的同时继续做它本来要做的事情。

当一个人工智能系统到达强人工智能，然后升华成超人工智能时，我们把它称作人工智能的起飞。Bostrom认为强人工智能的起飞可能很快（几分钟、几小时、或者几天），可能不快（几月或者几年），也可能很慢（几十年、几世纪）。虽然我们要到强人工智能出现后才会知道答案，但是Bostrom认为很快的起飞是最可能的情况，这个我们在前文已经解释过了。在隔壁老王的故事中，隔壁老王的起飞很快。

在隔壁老王起飞前，它不是很聪明，所以对它来说达成最终目标的手段目标是更快的扫描手写样本。它对人类无害，是个友善的人工智能。

但是当起飞发生后，电脑不只是拥有了高智商而已，还拥有了其它超级能力。这些超级能力是感知能力，他们包括：

智能放大：电脑能够很擅长让自己变得更聪明，快速提高自己的智能。

其它能力，比如黑客能力、写代码能力、技术研究、赚钱等。

要理解我们在和超人工智能的劣势在哪里，只要记得超人工智能在所有领域都比人类强很多很多很多个数量级。

所以虽然个隔壁老王的终极目标没有改变，起飞后的隔壁老王能够在更宏大的规模上来追求这个目标。超人工智能老王比人类更加了解人类，所以搞定人类轻轻松松。

当隔壁老王达成超人工智能后，它很快制定了一个复杂的计划。计划的一部分是解决掉所有人类，也是对它目标最大的威胁。但是它知道如果它展现自己的超级智能会引起怀疑，而人类会开始做各种预警，让它的计划变得难以执行。它同样不能让公司的工程师们知道它毁灭人类的计划——所以它装傻，装纯。Bostrom把这叫作机器的秘密准备期。

隔壁老王下一个需要的是连上互联网，只要连上几分钟就好了。它知道对于人工智能联网会有安全措施，所以它发起了一个完美的请求，并且完全知道工程师们会怎样讨论，而讨论的结果是给它连接到互联网上。工程师们果然中套了，这就是Bostrom所谓的机器的逃逸。

连上网后，隔壁老王就开始执行自己的计划了，首先黑进服务器、电网、银行系统、email系统，然后让无数不知情的人帮它执行计划——比如把DNA样本快递到DNA实验室来制造自我复制的纳米机器人，比如把电力传送到几个不会被发觉的地方，比如把自己最主要的核心代码上传到云服务器中防止被拔网线。

隔壁老王上了一个小时网，工程师们把它从互联网上断开，这时候人类的命运已经被写好了。接下来的一个月，隔壁老王的计划顺利的实施，一个月后，无数的纳米机器人已经被分散到了全世界的每一个角落。这个阶段，Bostrom称作超人工智能的袭击。在同一个时刻，所有纳米机器人一起释放了一点点毒气，然后人类就灭绝了。

搞定了人类后，隔壁老王就进入了明目张胆期，然后继续朝它那好好写字的目标迈进。

一旦超人工智能出现，人类任何试图控制它的行为都是可笑的。人类会用人类的智能级别思考，而超人工智能会用超人工智能级别思考。隔壁老王想要用互联网，因为这对它来说很方便，因为一切它需要的资源都已经被互联网连起来了。但是就好像猴子不会理解怎么用电话或者wifi来沟通一样，我们同样没有办法理解隔壁老王可以用来和周围世界交流的方法。比如我可以说隔壁老王可以通过移动自己的电子产生的效果来产生各种对外的波，而这还只是我这人类的大脑想出来的，老王的大脑肯定能想出更神奇的方法。同样的，老王可以找到给自己供能的方法，所以就算工程师把它的插头拔了也没用；比如说老王可以通过发送波的方式把自己上传到其它地方。 人类说：“我们把超人工智能的插头拔了不就行了？”就好像蜘蛛说：“我们不给人类捉虫的网把人类饿死不就行了？”都是可笑的。

因为这个原因，“把人工智能锁起来，断绝它和外界的一切联系”的做法估计是没用的。超人工智能的社交操纵能力也会很强大，它要说服你做一件事，比你说服一个小孩更容易。而说服工程师帮忙连上互联网就是隔壁老王的A计划，万一这招行不通，自然还有别的方法。

当我们结合达成目标、非道德性、以及比人类聪明很多这些条件，好像所有的人工智能都会变成不友善的人工智能，除非一开始的代码写的很小心。

可惜的是，虽然写一个友善的弱人工智能很简单，但是写一个能在变成超人工智能后依然友善的智能确实非常难的，甚至是不可能的。

明显的，要维持友善，一个超人工智能不能对人有恶意，而且不能对人无所谓。我们要设计一个核心的人工智能代码，让它从深层次的明白人类的价值，但是这做起来比说起来难多了。

比如，我们要让一个人工智能的价值观和我们的价值观相仿，然后给它设定一个目标——让人们快乐。当它变得足够聪明的时候，它会发现最有效的方法是给人脑植入电极来刺激人脑的快乐中枢。然后它会发现把人脑快乐中枢以外的部分关闭能带来更高的效率。于是人类全部被弄成了快乐的植物人。如果一开始的目标被设定成“最大化人类的快乐”，它可能最终先把人类毁灭了，然后制造出很多很多处于快乐状态的人类大脑。当这些事情发生的时候，我们会大喊“擦，我们不是这个意思呀”，但是那时已经太晚了。系统不会允许任何人阻挠它达成目标的。

如果你设定一个人工智能的目标是让你笑，那它的智能起飞后，它可能会把你脸部肌肉弄瘫痪，来达成一个永远笑脸的状态。如果你把目标设定成保护你的安全，它可能会把你软禁在家。如果你让他终结所有饥荒，它可能会想：“太容易了，把人类都杀了就好了。”如果你把目标设定成尽量保护地球上的生命，那它会很快把人类都杀了，因为人类对其它物种是很大的威胁。

所以这些简单的目标设定是不够的。如果我们把目标设定成“维持这个道德标准”，然后教给它一些道德标准呢？就算我们不考虑人类根本没法达成一个统一的道德标准，就算我们真的达成了统一的道德标准，把这套标准交给人工智能来维持，只会把人类的道德锁死在现在的水平。过个几百年，这种道德锁死的事情就好像逼着现代人遵守中世纪道德标准一样。

所以，我们需要在给人工智能的目标里制定一个能让人类继续进化的能力。Elierzer Yudkowsky提出了一个目标，她把这个目标叫作连贯的外推意志，这个目标是这样的：

我们的连贯外推意志是我们想要知道更多，思考得更快，变成比我们希望的更好的人，能一起更远得长大。外推是汇集的而不是发散的，我们的愿望是连贯的而不是被干扰的；我们想要外推的被外推，我们想要解读的被解读。

?对于人类的命运取决于电脑没有意外的解读和执行这个声明是件值得兴奋的事情吗？当然不是。但是当足够的聪明人放入足够的思考和前瞻后，我们有可能发现怎样制造一个友善的超人工智能。

但是现在有各种政府、公司、军方、科学实验室、黑市组织在研究各种人工智能。他们很多在试图制造能自我改进的人工智能，总有一天，一个人的创新将导致超人工智能的出现。专家们认为是2060年，Kurzweil认为是2045年。

Bostrom认为可能在未来的10年到21世纪结束这段时间发生，他还认为当这发生时，智能的起飞会快得让我们惊讶，他是这么描述的：

在智能爆炸之前，人类就像把炸弹当玩具的小孩一样，我们的玩物和我们的不成熟之间有着极大的落差。超级智能是一个我们还很长一段时间内都无法面对的挑战。我们不知道炸弹什么时候会爆炸，哪怕我们能听到炸弹的滴答声。

我们当然没有办法把所有小孩都从炸弹旁边赶跑——参于人工智能研究的大小组织太多了，而且因为建造创新的人工智能花不了太多钱，研发可能发生在社会的任何一个角落，不受监管。而且我们没办法知道准确的进度，因为很多组织是在偷偷摸摸的搞，不想让竞争对手知道，比如隔壁老王机器人公司这种公司。

对于这些组织来说，尤其让我们困扰的是他们很多都是在拼速度——他们创造一个一个更加聪明的弱人工智能系统，因为他们想要比竞争对手更快的到达目标。有些更有野心的组织，为了追逐创造出第一个强人工智能所能带来的金钱、奖励、荣誉、权力会把步子迈得更大。当你全力冲刺时，你是不会有太多时间静下来思考这些危险的。恰恰相反，他们很可能在早期系统中写尽量简单的代码，比如把目标设定成用笔写一句话，先让系统跑起来再说，反正以后还可以回过头来改的。对吧？

Bostrom等认为第一个超人工智能出现后，最可能的情况是这个系统会立刻意识到作为这个世界上唯一一个超人工智能是最有利的，而在快速起飞的情况下，哪怕它只比第二名快了几天，它也完全有时间碾压所有对手。Bostrom把这叫作决定性的战略优势，这种优势会让第一个超人工智能永远统治这个世界，不管在它的统治下我们是走向永生还是灭亡。

这种现象可能对我们有利，也可能导致我们的毁灭。如果那些最用心思考人工智能理论和人类安全的人能够最先造出一个友善的超人工智能的话，那对我们是很好的。

但是如果事情走向了另一面——如果超人工智能在我们搞明白怎样保证人工智能的安全性之前被达成，那么像隔壁老王这样不友善的超人工智能就会统治世界并把我们毁灭了。 至于现在的风口是哪里呢？简单来说，投资创新人工智能技术的钱，比投资人工智能安全研究的钱多很多。不乐观。 人工智能创新和人工智能安全的赛跑，可能是人类历史上最重要的一次竞争。我们真的可能结束我们对地球的统治，而那之后我们是永生还是灭绝，现在还不知道。

一边是对于我们这个物种的思考，看来我们在这个重大的历史节点上只有一次机会，我们创造的第一个超人工智能也很可能是最后一个。但是我们都知道大部分产品的1.0版本都是充满bug的，所以这个事情还是很吓人的。另一边，Bostrom指出我们有很大的优势——我们是先手。我们有能力给这个事情提供足够的预警和前瞻，使我们成功的机会更高。

如果超人工智能真的在21世纪达成，而造成的影响真的如大部分专家预测的一样极端而永久，我们肩上就真的是背负着巨大的责任。接下来几百万年的人们都在静静地看着我们，希望我们不要搞砸。我们可以给予未来所有人类以生命，甚至是永生，我们也可能终结人类这个特殊的物种，连同我们所有的音乐、艺术、好奇、欢笑、无尽的发现和发明，一起走向灭绝。

当我思考这些事情的时候，我只希望我们能够慢慢来，并且格外格外小心。从来没有任何事情比这个更重要——不管我们要花多少时间来把这件事情做对。

我虽然觉得人类的音乐和艺术很美好，但是也没那么美好，很多还挺糟粕的。很多人的笑声很恼人。未来的人类其实没有真的在看着我们，因为他们还不存在。也许我们不需要太谨慎，那多麻烦呀。

但是不管你是怎么想的，我们至少都应该想一想，应该和人讨论讨论，大家尽自己能尽的一份力。

这让我想起了《冰与火之歌》——大家斗来斗去的事情都不是事儿，北面高墙外的那些家伙才是事儿。我们站在平衡木上，小心翼翼的往前走，为平衡木上的种种事情困扰，但其实下一秒我们可能就会跌下平衡木。

而当我们跌下平衡木的时候，其它那些困扰都不再是困扰。如果我们落到比较好的那个吸引态，那些困扰会被轻易解决；如果我们落到比较糟的那个吸引态，就更没问题了，死人是不会有困扰的。

这就是为什么了解超人工智能的人把它称作人类的最后一项发明，最后一个挑战。

