---
layout: post
url: https://www.huxiu.com/article/21532
name: 罗超
time: 2013-10-16 08:28
title: 解读百度就“违反Robots协议”向360巨额索赔：一场数据争夺战
---
Robots抓取案根源是百度数据优势 笔者在今年1月6日便率先发现360内测360百科，大量词条从界面风格、到词条属性、到扩展阅读几乎保持一致。当时我推测360搜索在2013年的发展方向或将是：在产品线上，360搜索完全走百度的模式——从新闻、网页、问答、地图、音乐到视频等（软件和应用搜索是360特有的）。不过奇虎360后续陆续推出了“雷电手机搜索”“软件搜索”“良医搜索”以及“购物搜索”，实现与百度产品的差异化。 一方面综合搜索百度有先发优势，有着十多年的数据和技术积累，360想在这方面赶超几无可能；另一方面360爬取百度数据的做法，遭到后者多重打压：既有悬在头上的诉讼，也有重定向等技术手段。在360搜索结果点击百度知道、百科等页面，将被重定向，二次点击使得用户无法享受完整的搜索体验。 细心观察Google、360等搜索引擎会发现，首页结果出现百度知道、百科和贴吧内容的几率非常大，搜搜问问、爱问知识人和奇虎问答则是补充。如果搜索引擎没有百度的数据，用户找到想要的结果的几率会降低很多。 百度在2004年开始每年推出一个重量级产品：贴吧、知道和百科。现在百度的数据优势显示了当初UGC策略的英明之处。这些用户创造的数据已经成为百度的核心资产，同时百度官方运营人员也功不可没，而360直接将百度辛苦积攒的数据拿去使用，百度自然难以接受。数据是否丰富将很大程度决定搜索体验。 Robots协议，网站维护自身利益的工具？ Robots协议是网站站长与搜索引擎之间共同讨论后形成、通过Robots.txt落地。网站站长用它决定对搜索引擎的开放程度，引导爬虫如何更有效地爬取自己。现被广泛采用。Google、百度等搜索引擎均严格遵守。通常网站可以在服务器根目录下的“Robots.txt”中指明哪些内容可以被搜索引擎抓取，哪些不可以；也可以指明对那个搜索引擎开放，或者对哪个不开放。限制某个搜索引擎，Robots初衷是限制“BadRob”，即坏爬虫。所谓坏，是指存在安全或隐私问题，抑或太高频率爬取导致服务器压力。 事实上，Robots最初是用来约束搜索引擎的。搜索引擎梦想是获取所有数据，Robots限制了这一点。Robots也可以设置站点地图、屏蔽死链接以及减轻服务器压力不让爬虫爬取大文件。但整体而言搜索引擎是不欢迎Robots的，据某站长介绍，如何要想从搜索引擎获得更多流量，最好别用Robots文件。 不过百度是一家搜索公司，也是一家内容网站——当被Google、360等搜索引擎爬取时，李彦宏的角色就是网站站长。Robots协议对其也有保护作用。对360启用Robots限制很大程度是为了维护自身数据优势，防范竞争。Robots协议现在已逐步成为网站主维护利益的工具。 2008年淘宝屏蔽了Google、百度等搜索引擎也是利用Robots协议，理由是欺诈风险，今年淘宝屏蔽微信也是类似的理由。京东商城也通过Robots协议屏蔽了阿里旗下的购物搜索引擎一淘：因为一淘未经允许抓取京东商品评价，而这些评价花费了京东上亿的积分激励资源。屏蔽一淘得到苏宁易购的效仿。 360对百度不满意之处在于：百度的Robots采用了允许部分网站的方式，360被排除在外。其他搜索引擎例如搜狗就可以搜索百度内容。据接近百度内部人士介绍，搜索引擎要加入百度robots协议的白名单，一般需要与之签署一份书面协议。尽管360前几天与百度打了一场足球赛，但暂时应该还难以与之签署书面协议。 显而易见，百度屏蔽360、淘宝屏蔽百度、京东和苏宁易购屏蔽一淘，均是利用Robots协议来应对竞争对手，而不是因为对方的爬虫是“坏爬虫”。 用户创造内容的版权归属成为焦点 Robots的效力与“口头约定”差不多。但进入搜索引擎行业的均会遵循这个游戏规则，这得靠自律。但是违反协议本身是否被法律制裁，现在难以判断。如果争论焦点围绕著作权，届时还要看360的行为是否符合避风港原则。 360认为百度不应该将Robots协议这么用，他们抓取的数据是用户创造的，百度不应该屏蔽。并且百度不应该只对自己屏蔽。就算360觉得委屈，更合适的方式是推动Robots协议修订，并且说服业界接受，这很难。但现在360采用不遵守协议直接爬取的方法，有点“以暴制暴”的感觉：“规则不公平，抑或有人滥用来对付我，我就不遵守这个规则。” 用户创造内容是否可以不经过网站允许被抓取呢？百度用户创造的内容并没有明确的所有权归属。国内只有知乎等少数UGC社区有CC协议（知识共享），百度内容究竟是属于用户还是百度，UGC社区需要更加明确的版权协议。但360并不能因此就要求百度必须开放数据。况且这些数据的产生百度确实有所付出，例如运营、技术、软硬件资源等。 那么国外有无先例呢？在12年前，美国加州北部的联邦地方法院，eBay起诉Bidder's Edg案中，Bidder‘Edg违反Robots协议抓取eBay数据，BE败诉。但是在2011年4月微软向欧盟起诉Google，因为Google限制竞争对手的搜索引擎正常访问YouTube，微软却获胜了。 难以预测本次百度起诉360案结局怎么样，因为可以借鉴的先例也给出了不同的答案。不过本案结局势必会给接下来国内的互联网内容归属、非法律范畴协议纠纷值提供重大的借鉴意义。 本案也将很大程度影响360搜索此后的发展方向。2008年奇虎便推出了问答，但与百度知道还有差距。接下来360很可能会收购知乎这类内容社区，还会加强自有UGC社区建设，很简单，如果人才智力是互联网公司最有价值的流动资产，那么数据将是未来最重要的固定资产。360现在的数据集中在底层安全数据，应用型数据还太少。这是它冒着被索赔1亿元的风险去爬取别家数据的根本原因。 作者微博为@广州阿超，微信为SuperSofter

笔者在今年1月6日便率先发现360内测360百科，大量词条从界面风格、到词条属性、到扩展阅读几乎保持一致。当时我推测360搜索在2013年的发展方向或将是：在产品线上，360搜索完全走百度的模式——从新闻、网页、问答、地图、音乐到视频等（软件和应用搜索是360特有的）。不过奇虎360后续陆续推出了“雷电手机搜索”“软件搜索”“良医搜索”以及“购物搜索”，实现与百度产品的差异化。

一方面综合搜索百度有先发优势，有着十多年的数据和技术积累，360想在这方面赶超几无可能；另一方面360爬取百度数据的做法，遭到后者多重打压：既有悬在头上的诉讼，也有重定向等技术手段。在360搜索结果点击百度知道、百科等页面，将被重定向，二次点击使得用户无法享受完整的搜索体验。

细心观察Google、360等搜索引擎会发现，首页结果出现百度知道、百科和贴吧内容的几率非常大，搜搜问问、爱问知识人和奇虎问答则是补充。如果搜索引擎没有百度的数据，用户找到想要的结果的几率会降低很多。

百度在2004年开始每年推出一个重量级产品：贴吧、知道和百科。现在百度的数据优势显示了当初UGC策略的英明之处。这些用户创造的数据已经成为百度的核心资产，同时百度官方运营人员也功不可没，而360直接将百度辛苦积攒的数据拿去使用，百度自然难以接受。数据是否丰富将很大程度决定搜索体验。

Robots协议，网站维护自身利益的工具？

Robots协议是网站站长与搜索引擎之间共同讨论后形成、通过Robots.txt落地。网站站长用它决定对搜索引擎的开放程度，引导爬虫如何更有效地爬取自己。现被广泛采用。Google、百度等搜索引擎均严格遵守。通常网站可以在服务器根目录下的“Robots.txt”中指明哪些内容可以被搜索引擎抓取，哪些不可以；也可以指明对那个搜索引擎开放，或者对哪个不开放。限制某个搜索引擎，Robots初衷是限制“BadRob”，即坏爬虫。所谓坏，是指存在安全或隐私问题，抑或太高频率爬取导致服务器压力。

事实上，Robots最初是用来约束搜索引擎的。搜索引擎梦想是获取所有数据，Robots限制了这一点。Robots也可以设置站点地图、屏蔽死链接以及减轻服务器压力不让爬虫爬取大文件。但整体而言搜索引擎是不欢迎Robots的，据某站长介绍，如何要想从搜索引擎获得更多流量，最好别用Robots文件。

不过百度是一家搜索公司，也是一家内容网站——当被Google、360等搜索引擎爬取时，李彦宏的角色就是网站站长。Robots协议对其也有保护作用。对360启用Robots限制很大程度是为了维护自身数据优势，防范竞争。Robots协议现在已逐步成为网站主维护利益的工具。

2008年淘宝屏蔽了Google、百度等搜索引擎也是利用Robots协议，理由是欺诈风险，今年淘宝屏蔽微信也是类似的理由。京东商城也通过Robots协议屏蔽了阿里旗下的购物搜索引擎一淘：因为一淘未经允许抓取京东商品评价，而这些评价花费了京东上亿的积分激励资源。屏蔽一淘得到苏宁易购的效仿。

360对百度不满意之处在于：百度的Robots采用了允许部分网站的方式，360被排除在外。其他搜索引擎例如搜狗就可以搜索百度内容。据接近百度内部人士介绍，搜索引擎要加入百度robots协议的白名单，一般需要与之签署一份书面协议。尽管360前几天与百度打了一场足球赛，但暂时应该还难以与之签署书面协议。

显而易见，百度屏蔽360、淘宝屏蔽百度、京东和苏宁易购屏蔽一淘，均是利用Robots协议来应对竞争对手，而不是因为对方的爬虫是“坏爬虫”。

Robots的效力与“口头约定”差不多。但进入搜索引擎行业的均会遵循这个游戏规则，这得靠自律。但是违反协议本身是否被法律制裁，现在难以判断。如果争论焦点围绕著作权，届时还要看360的行为是否符合避风港原则。

360认为百度不应该将Robots协议这么用，他们抓取的数据是用户创造的，百度不应该屏蔽。并且百度不应该只对自己屏蔽。就算360觉得委屈，更合适的方式是推动Robots协议修订，并且说服业界接受，这很难。但现在360采用不遵守协议直接爬取的方法，有点“以暴制暴”的感觉：“规则不公平，抑或有人滥用来对付我，我就不遵守这个规则。”

用户创造内容是否可以不经过网站允许被抓取呢？百度用户创造的内容并没有明确的所有权归属。国内只有知乎等少数UGC社区有CC协议（知识共享），百度内容究竟是属于用户还是百度，UGC社区需要更加明确的版权协议。但360并不能因此就要求百度必须开放数据。况且这些数据的产生百度确实有所付出，例如运营、技术、软硬件资源等。

那么国外有无先例呢？在12年前，美国加州北部的联邦地方法院，eBay起诉Bidder's Edg案中，Bidder‘Edg违反Robots协议抓取eBay数据，BE败诉。但是在2011年4月微软向欧盟起诉Google，因为Google限制竞争对手的搜索引擎正常访问YouTube，微软却获胜了。

难以预测本次百度起诉360案结局怎么样，因为可以借鉴的先例也给出了不同的答案。不过本案结局势必会给接下来国内的互联网内容归属、非法律范畴协议纠纷值提供重大的借鉴意义。

本案也将很大程度影响360搜索此后的发展方向。2008年奇虎便推出了问答，但与百度知道还有差距。接下来360很可能会收购知乎这类内容社区，还会加强自有UGC社区建设，很简单，如果人才智力是互联网公司最有价值的流动资产，那么数据将是未来最重要的固定资产。360现在的数据集中在底层安全数据，应用型数据还太少。这是它冒着被索赔1亿元的风险去爬取别家数据的根本原因。

作者微博为@广州阿超，微信为SuperSofter

