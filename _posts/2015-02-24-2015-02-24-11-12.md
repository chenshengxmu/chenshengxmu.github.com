---
layout: post
url: https://www.huxiu.com/article/108990
name: Wait But Why
time: 2015-02-24 11:12
title: 为什么霍金、比尔·盖茨这些大佬们，让我们警惕人工智能？（AI革命上篇）
---
虎嗅注：本文原载于英文网站Wait But Why，文章内容来自原文作者Tim Urban的两篇文章：《The AI Revolution: Road to Superintelligence》和《The AI Revolution: Our Immortality or Extinction》。文章由知乎@谢熊猫君 翻译。由于篇幅较长，文章分为上下两篇编发。

本文是上篇，主要从三个层面依次递进论述：1.历史会呈指数发展，人工智能对历史的改变会很快到来。2.人工智能是什么，现在的人工智能发展水平如何。3.我们可以按照怎样的轨迹发展人工智能。

当人工智能发展到人类脑残水平时，他就可以自我发展了，并在很短的时间内发生——智能爆炸，超过人类水平。那时，就像人类重新定义了动物一样，人工智能将会重新定义人类。那么这个如上帝般强大的存在，会仁慈的对待他的“父母”——人类吗？更惊悚的内容我们会在下篇进行讨论。

我们正站在变革的边缘，而这次变革将和人类的出现一般意义重大 – Vernor Vinge 如果你站在这里，你会是什么感觉？ 看上去非常刺激吧？但是你要记住，当你真的站在时间的图表中的时候，你是看不到曲线的右边的，因为你是看不到未来的。所以你真实的感觉大概是这样的： 稀松平常。

想象一下坐时间机器回到1750年的地球，那个时代没有电，畅通通讯基本靠吼，交通主要靠动物拉着跑。你在那个时代邀请了一个叫老王的人到2015年来玩，顺便看看他对“未来”有什么感受。我们可能没有办法了解1750年的老王内心的感受——金属铁壳在宽敞的公路上飞驰，和太平洋另一头的人聊天，看几千公里外正在发生进行的体育比赛，观看一场发生于半个世纪前的演唱会，从口袋里掏出一个黑色长方形工具把眼前发生的事情记录下来，生成一个地图然后地图上有个蓝点告诉你现在的位置，一边看着地球另一边的人的脸一边聊天，以及其它各种各样的黑科技。别忘了，你还没跟他解释互联网、国际空间站、大型强子对撞机、核武器以及相对论。

这时候的老王会是什么体验？惊讶、震惊、脑洞大开这些词都太温顺了，我觉得老王很可能直接被吓尿了。

但是，老王也回到了250年前的1500年，想邀请生活在1500年的小李到1750年被吓尿。小李可能会被250年后的很多东西震惊，但是至少他不会被吓尿。同样是250来年的时间，1750和2015年的差别，比1500年和1750年的差别，要大得多了。

所以说，对于1750年的老王来说，要把人吓尿，他需要回到更古老的过去——比如回到公元前12000年，第一次农业革命之前。那个时候还没有城市，也还没有文明。一个来自狩猎采集时代的人类，只是当时众多物种中的一个罢了，来自那个时代的小赵看到1750年庞大的人类帝国，可以航行于海洋上的巨舰，居住在“室内”，无数的收藏品，神奇的知识和发现——他很有可能被吓尿。

所以，一个人去到未来，并且被吓尿，他们需要满足一个“吓尿单位”。满足吓尿单位所需的年代间隔是不一样的。在狩猎采集时代满足一个吓尿单位需要超过十万年，而工业革命后一个吓尿单位只要两百多年就能满足。

未来学家Ray Kurzweil把这种人类的加速发展称作加速回报定律（Law of Accelerating Returns）。之所以会发生这种规律，是因为一个更加发达的社会，能够继续发展的能力也更强，发展的速度也更快——这本就是更加发达的一个标准。19世纪的人们比15世纪的人们懂得多得多，所以19世纪的人发展起来的速度自然比15世纪的人更快。

进步越来越大，发生的越来越快，也就是说我们的未来会很有趣对吧？

未来学家Kurzweil认为整个20世纪100年的进步，按照2000年的速度只要20年就能达成——2000年的发展速度是20世纪平均发展速度的5倍。他认为2000年开始只要花14年就能达成整个20世纪一百年的进步，而之后2014年开始只要花7年（2021年），就能达到又一个20世纪一百年的进步。几十年之后，我们每年都能达成好几次相当于整个20世纪的发展，再往后，说不定每个月都能达成一次。按照加速回报定，Kurzweil认为人类在21世纪的进步将是20世纪的1000倍。

如果Kurzweil等人的想法是正确的，那2030年的世界可能就能把我们吓尿了——下一个吓尿单位可能只需要十几年，而2050年的世界会变得面目全非。 你可能觉得2050年的世界会变得面目全非这句话很可笑，但是这不是科幻，而是比你我聪明很多的科学家们相信的，而且从历史来看，也是逻辑上可以预测的。

1. 我们对于历史的思考是线性的。当我们考虑未来35年的变化时，我们参照的是过去35年发生的事情。当我们考虑21世纪能产生的变化的时候，我们参考的是20世纪发生的变化。这就好像1750年的老王觉得1500年的小李在1750年能被吓尿一样。线性思考是本能的，但是考虑未来的时候我们应该指数地思考。一个聪明人不会把过去35年的发展作为未来35年的参考，而是会看到当下的发展速度，这样预测的会更准确一点。当然这样还是不够准确，想要更准确，你要想象发展的速度会越来越快。

2. 近期的历史很可能对人产生误导。首先，即使是坡度很高的指数曲线，只要你截取的部分够短，看起来也是很线性的，就好像你截取圆周的很小一块，看上去就是和直线差不多。其次，指数增长不是平滑统一的，发展常常遵循S曲线。 S曲线发生在新范式传遍世界的时候，S曲线分三部分：

如果你只看近期的历史，你很可能看到的是S曲线的某一部分，而这部分可能不能说明发展究竟有多快速。1995-2007年是互联网爆炸发展的时候，微软、谷歌、脸书进入了公众视野，伴随着的是社交网络、手机的出现和普及、智能手机的出现和普及，这一段时间就是S曲线的快速增长期。2008-2015年发展没那么迅速，至少在技术领域是这样的。如果按照过去几年的发展速度来估计当下的发展速度，可能会错得离谱，因为很有可能下一个快速增长期正在萌芽。

3. 个人经验使得我们对于未来预期过于死板。我们通过自身的经验来产生世界观，而经验把发展的速度烙印在了我们脑中——“发展就是这么个速度的。”我们还会受限于自己的想象力，因为想象力通过过去的经验来组成对未来的预测——但是我们知道的东西是不足以帮助我们预测未来的。当我们听到一个和我们经验相违背的对于未来的预测时，我们就会觉得这个预测偏了。如果我现在跟你说你可以活到150岁，250岁，甚至会永生，你是不是觉得我在扯淡——“自古以来，所有人都是会死的。”是的，过去从来没有人永生过，但是飞机发明之前也没有人坐过飞机呀。

接下来的内容，你可能一边读一边心里“呵呵”，而且这些内容可能真的是错的。但是如果我们是真的从历史规律来进行逻辑思考的，我们的结论就应该是未来的几十年将发生比我们预期的多得多得多得多的变化。同样的逻辑也表明，如果人类这个地球上最发达的物种能够越走越快，总有一天，他们会迈出彻底改变“人类是什么”这一观点的一大步，就好像自然进化不不断朝着智能迈步，并且最终迈出一大步产生了人类，从而完全改变了其它所有生物的命运。如果你留心一下近来的科技进步的话，你会发现，到处都暗示着我们对于生命的认知将要被接下来的发展而彻底改变。

如果你一直以来把人工智能（AI）当做科幻小说，但是近来却不但听到很多正经人严肃的讨论这个问题，你可能也会困惑。这种困惑是有原因的：

1.我们总是把人工智能和电影想到一起。星球大战、终结者、2001：太空漫游等等。电影是虚构的，那些电影角色也是虚构的，所以我们总是觉得人工智能缺乏真实感。

2.人工智能是个很宽泛的话题。从手机上的计算器到无人驾驶汽车，到未来可能改变世界的重大变革，人工智能可以用来描述很多东西，所以人们会有疑惑。

3.我们日常生活中已经每天都在使用人工智能了，只是我们没意识到而已。John McCarthy，在1956年最早使用了人工智能（Artificial Intelligence）这个词。他总是抱怨“一旦一样东西用人工智能实现了，人们就不再叫它人工智能了。”

因为这种效应，所以人工智能听起来总让人觉得是未来的神秘存在，而不是身边已经存在的现实。同时，这种效应也让人们觉得人工智能是一个从未被实现过的流行理念。Kurzweil提到经常有人说人工智能在80年代就被遗弃了，这种说法就好像“互联网已经在21世纪初互联网泡沫爆炸时死去了”一般滑稽。

所以，让我们从头开始。

首先，不要一提到人工智能就想着机器人。机器人只是人工智能的容器，机器人有时候是人形，有时候不是，但是人工智能自身只是机器人体内的电脑。人工智能是大脑的话，机器人就是身体——而且这个身体不一定是必需的。比如说Siri背后的软件和数据是人工智能，Siri说话的声音是这个人工智能的人格化体现，但是Siri本身并没有机器人这个组成部分。

其次，你可能听过“奇点”或者“技术奇点”这种说法。这种说法在数学上用来描述类似渐进的情况，这种情况下通常的规律就不适用了。这种说法同样被用在物理上来描述无限小的高密度黑洞，同样是通常的规律不适用的情况。Kurzweil则把奇点定义为加速回报定律达到了极限，技术进步以近乎无限的速度发展，而奇点之后我们将在一个完全不同的世界生活的。但是当下的很多思考人工智能的人已经不再用奇点这个说法了，而且这种说法很容易把人弄混，所以本文也尽量少用。

最后，人工智能的概念很宽，所以人工智能也分很多种，我们按照人工智能的实力将其分成三大类。

弱人工智能Artificial Narrow Intelligence (ANI): 弱人工智能是擅长于单个方面的人工智能。比如有能战胜象棋世界冠军的人工智能，但是它只会下象棋，你要问它怎样更好地在硬盘上储存数据，它就不知道怎么回答你了。

强人工智能Artificial General Intelligence (AGI): 人类级别的人工智能。强人工智能是指在各方面都能和人类比肩的人工智能，人类能干的脑力活它都能干。创造强人工智能比创造弱人工智能难得多，我们现在还做不到。Linda Gottfredson教授把智能定义为“一种宽泛的心理能力，能够进行思考、计划、解决问题、抽象思维、理解复杂理念、快速学习和从经验中学习等操作。”强人工智能在进行这些操作时应该和人类一样得心应手。

超人工智能Artificial Superintelligence (ASI): 牛津哲学家，知名人工智能思想家Nick Bostrom把超级智能定义为“在几乎所有领域都比最聪明的人类大脑都聪明很多，包括科学创新、通识和社交技能。”超人工智能可以是各方面都比人类强一点，也可以是各方面都比人类强万亿倍的。超人工智能也正是为什么人工智能这个话题这么火热的缘故，同样也是为什么永生和灭绝这两个词会在本文中多次出现。

现在，人类已经掌握了弱人工智能。其实弱人工智能无处不在，人工智能革命是从弱人工智能，通过强人工智能，最终到达超人工智能的旅途。这段旅途中人类可能会生还下来，可能不会，但是无论如何，世界将变得完全不一样。

让我们来看看这个领域的思想家对于这个旅途是怎么看的，以及为什么人工智能革命可能比你想的要近得多。

弱人工智能是在特定领域等同或者超过人类智能/效率的机器智能，一些常见的例子：

汽车上有很多的弱人工智能系统，从控制防抱死系统的电脑，到控制汽油注入参数的电脑。谷歌正在测试的无人驾驶车，就包括了很多弱人工智能，这些弱人工智能能够感知周围环境并作出反应。

你的手机也充满了弱人工智能系统。当你用地图软件导航，接受音乐电台推荐，查询明天的天气，和Siri聊天，以及其它很多很多应用，其实都是弱人工智能。

垃圾邮件过滤器是一种经典的弱人工智能——它一开始就加载了很多识别垃圾邮件的智能，并且它会学习并且根据你的使用而获得经验。

智能室温调节也是一样，它能根据你的日常习惯来智能调节。

你在上网时候出现的各种其它电商网站的产品推荐，还有社交网站的好友推荐，这些都是弱人工智能的组成的，弱人工智能联网互相沟通，利用你的信息来进行推荐。网购时出现的“买这个商品的人还购买了”推荐，其实就是收集数百万用户行为然后产生信息来卖东西给你的弱人工智能。

谷歌翻译也是一种经典的人工智能——非常擅长单个领域。声音识别也是一种。很多软件利用这两种智能的合作，使得你能对着手机说中文，手机直接给你翻译成英文。

当飞机着陆时候，不是一个人类决定飞机该去那个登机口接驳。就好像你在网上买票时票据不是一个人类决定的。

谷歌搜索是一个巨大的弱人工智能，背后是非常复杂的排序方法和内容检索。社交网络的新鲜事同样是这样。 这些还只是消费级产品的例子。

军事、制造、金融（高频算法交易占到了美国股票交易的一半）等领域广泛运用各种复杂的弱人工智能。专业系统也有，比如帮助医生诊断疾病的系统，还有著名的IBM的华生，储存了大量事实数据，还能理解主持人的提问，在竞猜节目中能够战胜最厉害的参赛者。 现在的弱人工智能系统并不吓人。最糟糕的情况，无非是代码没写好，程序出故障，造成了单独的灾难，比如造成停电、核电站故障、金融市场崩盘等等。

虽然现在的弱人工智能没有威胁我们生存的能力，我们还是要怀着警惕的观点看待正在变得更加庞大和复杂的弱人工智能的生态。每一个弱人工智能的创新，都在给通往强人工智能和超人工智能的旅途添砖加瓦。用Aaron Saenz的观点，现在的弱人工智能，就是地球早期软泥中的氨基酸——没有动静的物质，突然之间就组成了生命。

只有明白创造一个人类智能水平的电脑是多么不容易，才能让你真的理解人类的智能是多么不可思议。造摩天大楼、把人送入太空、明白宇宙大爆炸的细节——这些都比理解人类的大脑，并且创造个类似的东西要简单太多了。至今为止，人类的大脑是我们所知宇宙中最复杂的东西。

而且创造强人工智能的难处，并不是你本能认为的那些。

造一个能在瞬间算出十位数乘法的计算机——非常简单 造一个能分辨出一个动物是猫还是狗的计算机——极端困难 造一个能战胜世界象棋冠军的电脑——早就成功了 造一个能够读懂六岁小朋友的图片书中的文字，并且了解那些词汇意思的电脑——谷歌花了几十亿美元在做，还没做出来。 一些我们觉得困难的事情——微积分、金融市场策略、翻译等，对于电脑来说都太简单了 我们觉得容易的事情——视觉、动态、移动、直觉——对电脑来说太TM的难了。

用计算机科学家Donald Knuth的说法，“人工智能已经在几乎所有需要思考的领域超过了人类，但是在那些人类和其它动物不需要思考就能完成的事情上，还差得很远。”

读者应该能很快意识到，那些对我们来说很简单的事情，其实是很复杂的，它们看上去很简单，因为它们已经在动物进化的过程中经历了几亿年的优化了。当你举手拿一件东西的时候，你肩膀、手肘、手腕里的肌肉、肌腱和骨头，瞬间就进行了一组复杂的物理运作，这一切还配合着你的眼睛的运作，使得你的手能都在三维空间中进行直线运作。对你来说这一切轻而易举，因为在你脑中负责处理这些的“软件”已经很完美了。同样的，软件很难识别网站的验证码，不是因为软件太蠢，恰恰相反，是因为能够读懂验证码是件碉堡了的事情。

同样的，大数相乘、下棋等等，对于生物来说是很新的技能，我们还没有几亿年的时间来进化这些能力，所以电脑很轻易的就击败了我们。试想一下，如果让你写一个程序，是一个能做大数相乘的程序容易写，还是能够识别千千万万种字体和笔迹下书写的英文字母的程序难写？

比如看着下面这个图的时候，你和电脑都能识别出这是一个由两种颜色的小长方形组成的一个大长方形。 你和电脑打了个平手。接着我们把途中的黑色部分去除： 你可以轻易的描述图形中透明或不透明的圆柱和3D图形，但是电脑就看不出来了。电脑会描述出2D的阴影细节，但是人脑却能够把这些阴影所展现的深度、阴影混合、房屋灯光解读出来。

再看下面这张图，电脑看到的是黑白灰，我们看到的却是一块全黑的石头。

而且，我们到现在谈的还是静态不变的信息。要想达到人类级别的智能，电脑必须要理解更高深的东西，比如微小的脸部表情变化，开心、放松、满足、满意、高兴这些类似情绪间的区别，以及为什么《布达佩斯大饭店》是好电影，而《富春山居图》是烂电影。

要达到强人工智能，肯定要满足的就是电脑硬件的运算能力。如果一个人工智能要像人脑一般聪明，它至少要能达到人脑的运算能力。

用来描述运算能力的单位叫作cps（calculations per second，每秒计算次数），要计算人脑的cps只要了解人脑中所有结构的最高cps，然后加起来就行了。

Kurzweil把对于一个结构的最大cps的专业估算，然后考虑这个结构占整个大脑的重量，做乘法，来得出人脑的cps。听起来不太靠谱，但是Kurzweil用了对于不同大脑区域的专业估算值，得出的最终结果都非常类似，是10^16 cps，也就是1亿亿次计算每秒。

现在最快的超级计算机，中国的天河二号，其实已经超过这个运算力了，天河每秒能进行3.4亿亿。当然，天河二号占地720平方米，耗电2400万瓦，耗费了3.9亿美元建造。广泛应用就不提了，即使是大部分商业或者工业运用也是很贵的。 Kurzweil认为考虑电脑的发展程度的标杆是看1000美元能买到多少cps，当1000美元能买到人脑级别的1亿亿运算能力的时候，强人工智能可能就是生活的一部分了。

摩尔定律认为全世界的电脑运算能力每两年就翻一倍，这一定律有历史数据所支持，这同样表明电脑硬件的发展和人类发展一样是指数级别的。我们用这个定律来衡量1000美元什么时候能买到1亿亿cps。现在1000美元能买到10万亿cps，和摩尔定律的历史预测相符合。

也就是说现在1000美元能买到的电脑已经强过了老鼠，并且达到了人脑千分之一的水平。听起来还是弱爆了，但是，让我们考虑一下，1985年的时候，同样的钱只能买到人脑万亿分之一的cps，1995年变成了十亿分之一，2005年是百万分之一，而2015年已经是千分之一了。按照这个速度，我们到2025年就能花1000美元买到可以和人脑运算速度抗衡的电脑了。

至少在硬件上，我们已经能够强人工智能了（中国的天河二号），而且十年以内，我们就能以低廉的价格买到能够支持强人工智能的电脑硬件。

但是运算能力并不能让电脑变得智能，下一个问题是，我们怎样利用这份运算能力来达成人类水平的智能。

这一步比较难搞。事实上，没人知道该怎么搞——我们还停留在争论怎么让电脑分辨《富春山居图》是部烂片的阶段。但是，现在有一些策略，有可能会有效。下面是最常见的三种策略：

就好像你班上有一个学霸。你不知道为什么学霸那么聪明，为什么考试每次都满分。虽然你也很努力的学习，但是你就是考的没有学霸好。最后你决定“老子不干了，我直接抄他的考试答案好了。”这种“抄袭”是有道理的，我们想要建造一个超级复杂的电脑，但是我们有人脑这个范本可以参考呀。

科学界正在努力逆向工程人脑，来理解生物进化是怎么造出这么个神奇的东西的，乐观的估计是我们在2030年之前能够完成这个任务。一旦这个成就达成，我们就能知道为什么人脑能够如此高效、快速的运行，并且能从中获得灵感来进行创新。一个电脑架构模拟人脑的例子就是人工神经网络。它是一个由晶体管作为“神经”组成的网络，晶体管和其它晶体管互相连接，有自己的输入、输出系统，而且什么都不知道——就像一个婴儿的大脑。接着它会通过做任务来自我学习，比如识别笔迹。最开始它的神经处理和猜测会是随机的，但是当它得到正确的回馈后，相关晶体管之间的连接就会被加强；如果它得到错误的回馈，连接就会变弱。经过一段时间的测试和回馈后，这个网络自身就会组成一个智能的神经路径，而处理这项任务的能力也得到了优化。人脑的学习是类似的过程，不过比这复杂一点，随着我们对大脑研究的深入，我们将会发现更好的组建神经连接的方法。

更加极端的“抄袭”方式是“整脑模拟”。具体来说就是把人脑切成很薄的片，用软件来准确的组建一个3D模型，然后把这个模型装在强力的电脑上。如果能做成，这台电脑就能做所有人脑能做的事情——只要让它学习和吸收信息就好了。如果做这事情的工程师够厉害的话，他们模拟出来的人脑甚至会有原本人脑的人格和记忆，电脑模拟出的人脑就会像原本的人脑一样——这就是非常符合人类标准的强人工智能，然后我们就能把它改造成一个更加厉害的超人工智能了。

我们离整脑模拟还有多远呢？至今为止，我们刚刚能够模拟1毫米长的扁虫的大脑，这个大脑含有302个神经元。人类的大脑有1000亿个神经元，听起来还差很远。但是要记住指数增长的威力——我们已经能模拟小虫子的大脑了，蚂蚁的大脑也不远了，接着就是老鼠的大脑，到那时模拟人类大脑就不是那么不现实的事情了。

抄学霸的答案当然是一种方法，但是如果学霸的答案太难抄了呢？那我们能不能学一下学霸备考的方法？

首先我们很确定的知道，建造一个和人脑一样强大的电脑是可能的——我们的大脑就是证据。如果大脑太难完全模拟，那么我们可以模拟演化出大脑的过程。事实上，就算我们真的能完全模拟大脑，结果也就好像照抄鸟类翅膀的拍动来造飞机一样——很多时候最好的设计机器的方式并不是照抄生物设计。

所以我们可不可以用模拟演化的方式来造强人工智能呢？这种方法叫作“基因算法”，它大概是这样的：建立一个反复运作的表现/评价过程，就好像生物通过生存这种方式来表现，并且以能否生养后代为评价一样。一组电脑将执行各种任务，最成功的将会“繁殖”，把各自的程序融合，产生新的电脑，而不成功的将会被剔除。经过多次的反复后。这个自然选择的过程将产生越来越强大的电脑。而这个方法的难点是建立一个自动化的评价和繁殖过程，使得整个流程能够自己运行。

这个方法的缺点也是很明显的，演化需要经过几十亿年的时间，而我们却只想花几十年时间。

但是比起自然演化来说，我们有很多优势。首先，自然演化是没有预知能力的，它是随机的——它产生的没用的变异比有用的变异多很多，但是人工模拟的演化可以控制过程，使其着重于有益的变化。其次，自然演化是没有目标的，自然演化出的智能也不是它目标，特定环境甚至对于更高的智能是不利的（因为高等智能消耗很多能源）。但是我们可以指挥演化的过程超更高智能的方向发展。再次，要产生智能，自然演化要先产生其它的附件，比如改良细胞产生能量的方法，但是我们完全可以用电力来代替这额外的负担。所以，人类主导的演化会比自然快很多很多，但是我们依然不清楚这些优势是否能使模拟演化成为可行的策略。

如果抄学霸的答案和模拟学霸备考的方法都走不通，那就干脆让考题自己解答自己吧。这种想法很无厘头，确实最有希望的一种。

总的思路是我们建造一个能进行两项任务的电脑——研究人工智能和修改自己的代码。这样它就不只能改进自己的架构了，我们直接把电脑变成了电脑科学家，提高电脑的智能就变成了电脑自己的任务。

硬件的快速发展和软件的创新是同时发生的，强人工智能可能比我们预期的更早降临，因为：

1）指数级增长的开端可能像蜗牛漫步，但是后期会跑的非常快。

2）软件的发展可能看起来很缓慢，但是一次顿悟，就能永远改变进步的速度。就好像在人类还信奉地心说的时候，科学家们没法计算宇宙的运作方式，但是日心说的发现让一切变得容易很多。创造一个能自我改进的电脑来说，对我们来说还很远，但是可能一个无意的变动，就能让现在的系统变得强大千倍，从而开启朝人类级别智能的冲刺。

总有一天，我们会造出和人类智能相当的强人工智能电脑，然后人类和电脑就会平等快乐的生活在一起。

呵呵，逗你呢。

即使是一个和人类智能完全一样，运算速度完全一样的强人工智能，也比人类有很多优势：

-速度。脑神经元的运算速度最多是200赫兹，今天的微处理器就能以2G赫兹，也就是神经元1000万倍的速度运行，而这比我们达成强人工智能需要的硬件还差远了。大脑的内部信息传播速度是每秒120米，电脑的信息传播速度是光速，差了好几个数量级。

- 容量和储存空间。人脑就那么大，后天没法把它变得更大，就算真的把它变得很大，每秒120米的信息传播速度也会成为巨大的瓶颈。电脑的物理大小可以非常随意，使得电脑能运用更多的硬件，更大的内存，长期有效的存储介质，不但容量大而且比人脑更准确。

- 可靠性和持久性。电脑的存储不但更加准确，而且晶体管比神经元更加精确，也更不容易萎缩（真的坏了也很好修）。人脑还很容易疲劳，但是电脑可以24小时不停的以峰值速度运作。

- 可编辑性，升级性，以及更多的可能性。和人脑不同，电脑软件可以进行更多的升级和修正，并且很容易做测试。电脑的升级可以加强人脑比较弱势的领域——人脑的视觉元件很发达，但是工程元件就挺弱的。而电脑不但能在视觉元件上匹敌人类，在工程元件上也一样可以加强和优化。

- 集体能力。人类在集体智能上可以碾压所有的物种。从早期的语言和大型社区的形成，到文字和印刷的发明，再到互联网的普及。人类的集体智能是我们统治其它物种的重要原因之一。而电脑在这方面比我们要强的很多，一个运行特定程序的人工智能网络能够经常在全球范围内自我同步，这样一台电脑学到的东西会立刻被其它所有电脑学得。而且电脑集群可以共同执行同一个任务，因为异见、动力、自利这些人类特有的东西未必会出现在电脑身上。

通过自我改进来达成强人工智能的人工智能，会把“人类水平的智能”当作一个重要的里程碑，但是也就仅此而已了。它不会停留在这个里程碑上的。考虑到强人工智能之于人脑的种种优势，人工智能只会在“人类水平”这个节点做短暂的停留，然后就会开始大踏步向超人类级别的智能走去。

这一切发生的时候我们很可能被吓尿，因为从我们的角度来看， a)虽然动物的智能有区别，但是动物智能的共同特点是比人类低很多；b)我们眼中最聪明的人类要比最愚笨的人类要聪明很很很很多。 所以，当人工智能开始朝人类级别智能靠近时，我们看到的是它逐渐变得更加智能，就好像一个动物一般。然后，它突然达到了最愚笨的人类的程度，我们到时也许会感慨：“看这个人工智能就跟个脑残人类一样聪明，真可爱。”

但问题是，从智能的大局来看，人和人的智能的差别，比如从最愚笨的人类到爱因斯坦的差距，其实是不大的。所以当人工智能达到了脑残级别的智能后，它会很快变得比爱因斯坦更加聪明： 之后呢？

从这边开始，这个话题要变得有点吓人了。我在这里要提醒大家，以下所说的都是大实话——是一大群受人尊敬的思想家和科学家关于未来的诚实的预测。你在下面读到什么离谱的东西的时候，要记得这些东西是比你我都聪明很多的人想出来的。

像上面所说的，我们当下用来达成强人工智能的模型大多数都依靠人工智能的自我改进。但是一旦它达到了强人工智能，即使算上那一小部分不是通过自我改进来达成强人工智能的系统，也会聪明到能够开始自我改进。

这里我们要引出一个沉重的概念——递归的自我改进。这个概念是这样的：一个运行在特定智能水平的人工智能，比如说脑残人类水平，有自我改进的机制。当它完成一次自我改进后，它比原来更加聪明了，我们假设它到了爱因斯坦水平。而这个时候它继续进行自我改进，然而现在它有了爱因斯坦水平的智能，所以这次改进会比上面一次更加容易，效果也更好。第二次的改进使得他比爱因斯坦还要聪明很多，让它接下来的改进进步更加明显。如此反复，这个强人工智能的智能水平越长越快，直到它达到了超人工智能的水平——这就是智能爆炸，也是加速回报定律的终极表现。 现在关于人工智能什么时候能达到人类普遍智能水平还有争议。对于数百位科学家的问卷调查显示他们认为强人工智能出现的中位年份是2040年——距今只有25年。这听起来可能没什么，但是要记住，很多这个领域的思想家认为从强人工智能到超人工智能的转化会快得多。以下的情景很可能会发生：一个人工智能系统花了几十年时间到达了人类脑残智能的水平，而当这个节点发生的时候，电脑对于世界的感知大概和一个四岁小孩一般；而在这节点后一个小时，电脑立马推导出了统一广义相对论和量子力学的物理学理论；而在这之后一个半小时，这个强人工智能变成了超人工智能，智能达到了普通人类的17万倍。

这个级别的超级智能不是我们能够理解的，就好像蜜蜂不会理解凯恩斯经济学一样。在我们的语言中，我们把130的智商叫作聪明，把85的智商叫作笨，但是我们不知道怎么形容12952的智商，人类语言中根本没这个概念。 但是我们知道的是，人类对于地球的统治教给我们一个道理——智能就是力量。也就是说，一个超人工智能，一旦被创造出来，将是地球有史以来最强大的东西，而所有生物，包括人类，都只能屈居其下——而这一切，有可能在未来几十年就发生。

想一下，如果我们的大脑能够发明Wifi，那么一个比我们聪明100倍、1000倍、甚至10亿倍的大脑说不定能够随时随地操纵这个世界所有原子的位置。那些在我们看来超自然的，只属于全能的上帝的能力，对于一个超人工智能来说可能就像按一下电灯开关那么简单。防止人类衰老，治疗各种不治之症，解决世界饥荒，甚至让人类永生，或者操纵气候来保护地球未来的什么，这一切都将变得可能。同样可能的是地球上所有生命的终结。

当一个超人工智能出生的时候，对我们来说就像一个全能的上帝降临地球一般。

这时候我们所关心的就是，这是否会是一个仁慈的上帝？

这篇文章的第一部分完了，我建议你休息一下，喝点水，下面我们要开始第二部分。

