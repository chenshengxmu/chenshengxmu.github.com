---
layout: post
url: https://www.huxiu.com/article/146216
name: 新智元
time: 2016-04-21 21:29
title: 英特尔全球裁员1.2万，AI时代CPU正被拉下王座？
---
新智元编译，译者：闻菲、胡祥杰

2016年4月19日，全球最大半导体厂商Intel宣布计划在全球范围内裁员1.2万人，相当于公司总人数的11%，这是近10年来Intel最大规模一次裁员。同时，这一举措还仅仅是Intel接下来将要进行的价值12亿美元公司重组计划的一部分。官方新闻称这是为了调整公司结构以应对后PC时代。在这个重磅新闻中，有一个关键词可能被忽略了，那就是手机。

据媒体报道，Intel将在全球裁员11%，以“推动公司从一家个人电脑公司向云以及10多亿智能联结计算设备的公司转型”。但是，这里的智能并不是指智能手机，或者说，至少看起来不是。

在这里，“智能联结计算设备”指的是物联网。 当然，物联网这一被用烂的词并没能完全抓住Intel真正想要做的全部事情（这不仅仅是用让联网的温度控制器那么简单）。不管怎么说，Intel事实上已经不再是一家PC公司或者一家智能手机公司。并且，物联网还没有真正成型，所以，Intel要变成一家云公司。

这对Intel来说是件好事，至少目前看来是这样。不管你认为云的名字该叫什么，这个市场是确实存在的，并且Intel也处于统治地位。根据弗雷斯特研究公司的预测，到2020年，云计算的市场将会达到1910亿美元。云计算是一种开展业务的方式，能让企业在没有建立自己的服务器的情况下运行大量的软件。

Intel提供了几乎世界上所有的云服务机器的芯片，其中包括亚马逊、谷歌和微软。

鉴于PC市场正在消退，并且在智能手机芯片市场上几乎没有真正的成功过，现在Intel决定进行重构（重建品牌），把自己打造成一家服务于云计算的公司，或者像媒体报道的那样，一家为数据中心提供服务的芯片公司。

不可否认的是，Intel确实统治着所有的数据中心，而不仅仅是现在被热议的用来驱动云服务的这一个。根据研究公司IDC的报道，在支持计算机服务的芯片市场，Intel的占有率达到了99%。所以，就目前看来，这就是Intel的未来。

Intel看到了消费者正在流动（从PC到手机和平板），也看到了生意正在流动（寻求向所有的手机和平板提供联网服务导致的向数据中心的流动）。但是，与此同时，Intel也意识到，自己的优势并不在移动设备上。当然，公司依然在尝试推动移动芯片和蜂窝网络调制解调器的发展，但是至少，它知道不应该再自称是一家手机公司，特别是解聘了那么多员工。那是他们在很久以前就错过了的一个机会。

谈到物联网，Intel其实并没有落后。但其中的原因是物联网还没有真正的普及，并且很难预测物联网的未来会是怎么样的。至少短期来说是这样。“物联网领域还没诞生强者”，Moor Insights & Strategy的总裁兼首席分析师Patrick Moorhead说。Moor Insights & Strategy是一家紧密追踪芯片产业，尤其是Intel业务的研究公司。简而言之，Intel说 “数据中心和物联网业务是Intel的主要增长业务时”，它很可能说的是实话。

但是，即便是在数据中心领域，从某种程度上来说，Intel面临的是一个不确定的未来。现在，互联网巨头，其中包括很多大型的云计算公司，都在迅速地向人工智能领域的的一个分支——深度学习靠拢。这种人工智能技术现在推动图像识别和语音识别在谷歌搜索引擎中的应用。深度学习依赖于芯片，也就是常说的，图像处理单元（GPUs），这种芯片Intel没有销售。

虽然Intel确实提供了一种替代品FPGA。但是今天，GPUs是AI的支柱。随着深度学习技术在互联网以及云计算服务中的普及，GPUs制造商，比如Intel的死敌英伟达的影响力只会持续变强。这么看来，Intel的重构就可以理解了，但是它在云上的地位还远没有得到保证。

再来看Intel的最大竞争对手TSMC，这家芯片制造商在2015年量产N16，专家预计从2017年第二季度开始TSMC的N10技术就能带来显著盈利，并且TSMC计划在2018年上半年量产N7芯片。相对的，Intel在2014年中才开始量产N14，并且预计到2017年下半年才会过渡到N10技术。而Intel的N7更是要到2020年才会开始生产。也就是说，TSMC能够每年都实现大幅技术提升，而Intel没有做到这一点。

此外，近年来随着深度学习应用大量涌现，超级计算机的架构逐渐向深度学习应用优化，从传统CPU为主GPU为辅的Intel处理器变为GPU为主CPU为辅的结构。2016年3月，GUP巨头NVIDIA推出一款致力于加速人工智能和深度学习的芯片Tesla P100，投入研发经费20多亿美元。当时，Facebook人工智能实验室负责人Yann LeCun就评论称：“随着神经网络越变越大，我们不仅需要内存更大、速度更快的GPU，也需要大幅提升GPU间的通信速度以及能够利用降低精度进行运算的硬件。”

但是，“未来NVIDIA的这个市场仍面临低成本专用处理器和FPGA的冲击，”新智元智库专家、寒武纪科技创始人兼CEO陈天石告诉新智元记者。寒武纪科技的首个深度学习专用处理器架构——寒武纪神经网络处理器就只聚焦人工智能。

而来自低成本FPGA的威胁更是已经持续了很多年——2012年，百度决定自主设计深度学习专有的体系结构和芯片，经过深入研究和论证，为让项目快速落地及迭代，工程师最后决定使用FPGA实现百度第一版自主设计的深度学习专有芯片。

2016年4月16日，MIT Techonolgy Review报道，DARPA投资了一款叫做“S1”的概率芯片（Probabilistic Chip），在模拟测试中使用S1芯片追踪视频里的移动物体（比如汽车），每帧处理速度比传统处理器快了将近100倍，而能耗还不到传统处理器的2%。对此，MIT媒体实验室教授、Twitter首席媒体科学家Deb Roy评论称，越来越多的程序员要从图像和视频中采集数据，还要让机器理解现实世界和人的行为，无疑近似计算的潮流正在兴起。实际上，早在2008年MIT Techonolgy Review的“十大科技突破”预测中，概率芯片就榜上有名。

美国Singular Computing公司开发的“S1”概率芯片，获得DARPA投资，能够让计算机更好地分析图像。（来源：MIT Techonolgy Review）

差不多在同样的时间，IBM 发布了一款用于深度学习的类脑超级计算平台 IBM TrueNorth，基础是16个IBM此前研发的脉冲神经网络芯片TrueNorth，处理能力相当于 1600 万个神经元和 40 亿个神经键，消耗的能量只需 2.5 瓦。像这样将低能耗的类脑处理器应用于深度学习无疑是未来大数据处理的方向之一。

总之，Intel大幅裁员不仅是市场对PC需求量不断下降所致，我们能从中看出整个芯片产业一股更大的趋势：随着以深度学习为代表的人工智能技术走向主流，占据市场几十年的CPU可能被拉下王座；成本更低的FPGA、能够以更快速度处理数据的GPU、能够以更低精度进行计算的概率芯片和更多采用全新架构的专用处理器争夺市场的时代到来了。

Cade Metz，Intel Can’t Win Mobile, But It Owns the Cloud—For Now，Wired

Tom Simonite，Why a Chip That’s Bad at Math Can Help Computers Tackle Harder Problems，MIT Techonolgy Review

Ashraf Eassa，Intel Corporation Needs to Rethink Its Entire Silicon Strategy，Fool.com

