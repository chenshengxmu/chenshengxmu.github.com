---
layout: post
url: https://www.huxiu.com/article/143219
name: dongfeiwww
time: 2016-03-26 10:17
title: Google首席科学家谈Google是怎么做深度学习的
---
2016年3月7日，谷歌首席科学家，MapReduce、BigTable等系统的创造者，Jeff Dean受邀韩国大学，演讲主题《大规模深度学习》，这里部分来自highscalability的文字和笔者Youtube上的听录。刚好演讲在AlphaGo和李世石比赛之前，观众也问了他的预测，他只是说训练了5个月的机器跟顶尖高手的差距不好说；还有人问道他喜欢的编程语言（C++爱恨交织，喜欢Go的简洁，Sawzall才是真爱）；在Google作为首席一天是怎么过的（要吃好早饭，到处闲逛闲聊，找到那些每个领域专家一起攻克难题）。本文首发微信公众号“董老师在硅谷”，转载请联系。

如果你不理解信息中的奥秘，那么你也很难去组织它。

Jeff Dean是Google系统架构组院士，在讲座：“大规模深度学习构建智能计算机系统”中提到这句和Google的使命：整合全球信息，使人人皆可访问并从中受益。早期他们通过收集，清理，存储，索引，汇报，检索数据完成“整合”的工作，当Google完成这个使命，就去迎接下一个挑战。

看到这张图，你马上知道是小宝宝抱着泰迪熊睡觉。而看到下张街景，马上意识到纪念品店里面有打折信息。其实直到最近，计算机才可以提取图片中的信息。

如果想从图像去解释物理世界，计算机需要去选择跟那些感兴趣的点，阅读文字并去真正理解。

像下面的文字“car parts for sale”，传统的Google通过关键字匹配来给出结果，但更好的匹配是第二个。这是一个需求深度理解的过程，而不能停留在字面，要去做一个优秀搜索和语言理解产品。

Google跟其他公司的不同是，2011年就开始Google大脑计划，当时想通过使用神经网络来提升技术水准，但并没有把研究做成像大学象牙塔那种，而是结合安卓，Gmail，图片去改进产品解决真正问题。这对其他公司也是很好的借鉴，把研究和员工工作结合起来。

神经网络老早就开始研究，上世纪60年代发明，在80年代和90年代早期也流行过，后来又不火了。两个原因：缺少计算能力去训练数据模型，这样也不能用来做更大规模的问题；2）缺少大量有效的数据集。而Google通过算法的力量，在加上他们强大的基础架构，海量数据集创造了AI的绝佳温床。

深度学习一开始从少数的产品组开始，一段时间后反响很好，能解决之前不能做的，就更多的团队开始采纳。使用深度学习的产品有：安卓，Apps，药品发现，Gmail，图片理解，地图，自然语言，图片，机器人，语音翻译等。

深度学习能应用到很多领域原因是那些通用模块：语音，文字，搜索词，图片，视频，标签，实体，短语，音频特性。输入一类信息，决定你想要的输出，收集训练数据作为你想要计算的潜在函数，然后就放手不管了。

模型很赞的原因是因为灌了很多原始形式的数据。你不需要教工程师很多特征点，模型的力量在于从观察一些例子就能自动识别数据中的有用信息。

神经网络就是一些从数据提炼的复杂函数。从一个空间输入在转化为另一个空间的输出。这里的函数不是像平方，而是真正复杂的函数。当你给出一些原始像素，比如猫，而输出就是对象的类别。

比如你给输入一张猫的图片，输出是人工标记的猫图片，这是 监督学习。你把很多这样监督样本给系统，让它去学习近似的函数，如同从监督样本中观察出来的。

还有一种是非监督学习，给出一个图片，你也不知道里面是啥，系统可以学习去寻找在很多图片中出现的模式。这样即使不认识图片，它也能识别所有的图片中都有一只猫。

增强学习也适用，这也是AlphaGo用到的技术。

神经元有一组输入。真正神经元会有不同的强度的输入。在人工智能网中试图去学习到这些边上的权重，去加强不同输入的联系。真正神经元通过输入和强度的组合去决定要不要生成脉冲。

人工神经元不会产生脉冲，但会生成一个数值。神经元的函数就是通过非线性函数计算输入的加权乘以权重之和。

典型的非线性函数就是整形线性单元（max(0, x))，在90年代很多非线性函数是很平缓的sigmoid()函数或者tanh()函数。但对于神经元来说产生的数值是不是更接近0对优化系统更有利。比如如果神经元有3个输入 X1, X1, X3，权重分别是 -0.21, 0.3, 0.7,计算就是

为了决定图片到底是猫还是狗，这个图片要经过很多层。这些神经元根据输入来产生下一步。

这个模型也会错，比如说这里是猫，但事实上是狗。那么做错误决定的信号就会返回到系统中做调整，让剩余的模型在下一次查看图片时候，更可能输出狗。这就是神经网络的目标，通过模型小步调整边的权重让它更可能去得到正确答案。你可以通过所有样本去聚合，这样可以降低错误率。

选择随机训练样本“（输入，标签）”，比如上面猫图和想要的输出标签，‘猫’

运行神经网络，在输入上去查看它产生的。

反向传播：积分的链式法则在决定高层神经网络中使用，如果选择是猫而不是狗呢？得想办法去调整高层的权重去让它更可以决定是“狗”。

根据箭头方向和权重去让它更可能说是狗。不要步子迈得太大因为这种表面很复杂，微调一小步让它下次更可能给出狗的结果。通过很多迭代以及查看例子，结果更可能会是狗。通过这个链式法则去理解底层参数改变是如何影响到输出的。说白了就是网络变化回路反馈到输入，使得整个模型更适应去选择“狗”。

真正神经网络通过亿级的参数在亿级的维度做调整，去理解输出网络。Google目前有能力如何快速搭建和训练这些海量数据上的模型，去解决实际问题，在快速去不同广泛的平台去部署生产模型（手机，传感器，云端等）。

文本：万亿级别的英文和其他语言资料。从一个语言翻译到另一个，从短语到整句。

虚拟化数据：十亿级别的图谱，视频。

用户行为: 很多应用产生数据。比如搜索引擎的查询，用户在email中标记垃圾。这些都可以学习并搭建智能系统。

如果吸收更多数据，让模型变大，结果也更好。

如果你输入更多数据，但没有把模型变大，模型的能力就会受限在一些数据集中的明显特征。通过增加模型的规模，让它不仅记住明显的，还有一些也许出现很少的细微特征。

通过更大的模型，更多数据，计算需求也更大。Google很多精力花在如何提升计算量，训练更大的模型。

第一个部署深度神经网络的小组。他们实现的新模型基于神经网络而不是隐马尔可夫模型。这个问题是把从150毫秒的语音去预测中间10毫秒吞掉的声音。比如到底是ba还是ka的声音。你得到一个预测的序列，再通过语言模型去理解用户所说。

一开始的版本就把识别错误率降低了30%，确实非常厉害。后来就研究一些复杂模型去加强网络，进一步降低错误率。现在当你对着电话说话，语音识别比五年前强多了。

ImageNet是6年前公布的。里面有100万张图片，算是计算机视觉领域最大的。图片中包含1000种不同分类，每一类有1000张图片。比如里面有上千张不同的豹子，摩托车等，一个麻烦的是不是所有的标签都是对的。

在神经网络使用之前，最好的错误记录是26%，2014年 Google错误率暴降到6.66%取得冠军，然后到了2015年错误率下降到3.46%。这是什么概念，大家注意到Andrej人类的错误率也有5.1%（他还是花了24小时训练后的结果）。

总之这是个又大又深的模型，每个盒子就像神经元的一层去进行卷积操作。

计算机在花卉识别上很强大，这是非常好的模型，能够识别细微差别。

一般的效果，比如在菜品识别。

计算机也有犯错的时候，关于错误敏感性看一看上面的，比如左边鼻涕虫当成蛇，右边也不知道是什么鬼。

理解图片中像素的能力，Google图片团队开发了不用标签就可以搜索图片的功能。比如你可以去找雕像，素描，水，而不需提前标注。

在街景中如何识别里面的文字。首先要找到文字部分，模型能够去有效预测像素中热点图，那些含有文字的像素点。训练的数据就是包含文字划分的多边形。

因为训练数据中包括不同的字符集，这样在多语言下也没问题。也要考虑大小字体，远近，不同颜色。训练的模型相对容易，就是卷积神经网络尝试去预测每个像素是否包括文字。

RankBrain2015年启动，在搜索排名（前100位排第三），里面难点是搜索排序需要了解模型，要理解为什么要做某个决定。当系统发生错误为什么做那个。

调试工具准备好，需要足够的理解能力嵌入模型，去避免主观。总体上是不想手工调参数。你需要尝试理解模型中的预测，去理解训练数据是否相关，是否跟问题无关？你需要训练数据并应用到别的上面。通过搜索查询的分布你能得到每天的变化，事件发生后改变也随时发生。你要看分布是否稳定，比如语音识别，一般人不会改变音色。当查询和文档内容频繁变化，你要保证模型是新的。我们要搭建通用工具去理解神经网络里面发生了什么，解释什么导致这个预测。

很多问题都可以映射到从一个序列到另一个序列的规律。比如语言翻译，从英语翻译到法语，就是把英语的序列单词转化到法语序列单词。

神经网络在学习复杂函数时特别有用，这个模型学习从英文到法文的句子。句子以单词为单位，以结束符作为信号。训练模型在遇到结束符时开始产生另一个语言的对应句子。而模型函数就是把语言中语句对作为训练数据。

每一步都在词典表中的单词产生概率分布。在推理时候通过一些搜索来实现，如果你最大化每个单词的概率，这样找的不是最可能的句子。直到找到最大可能的句子找到才结束搜索。

这个系统在公开翻译系统中表现出色。大多数其他翻译系统需要手工编码或机器学习的模型只是在一小部分使用，而不是像这种整体的端到端的学习系统。

智能回复是另一个序列类的例子。在手机上你如何更快回复邮件，打字很累。

Gmail组开发了一个系统能够去预测邮件回复。第一步就是训练小量模型去预测如果消息是某一类的，怎么做简短回复。如果是一个更大，计算能力更强的模型将消息作为一个序列，尝试预测序列的响应语。比如对于节日邀约，最可能的三个答复是“算上我们”，“我们会去的”，“对不起，我们有事没法耍”。

把之前开发的图片模型与序列类模型结合一起。图片模型作为输入。这里就不是阅读英文句子了，而是看图片的像素。

接下来就是训练生成字幕。训练集有5个由不同的人写的不同的字幕。总共100万图片，70万条语句。效果如下

上面是一些好玩的出错语句，为啥会错，其实你自己看了也明白。

翻译团队写了一个app，使用计算机视觉来识别镜头中文字，再翻译成文本，最后再图片本身覆盖翻译好的文字。模型足够小可以运行在所有设备上。

直接在手机上跑一些模型中的重要方法。智能化将转移到设备端，这样不会依赖远程云端的大脑。

Google 非常在乎研究转化效率。就是要快速训练模型，理解那些做的好的和不好的，再想下一步实验。模型应该再分钟或者小时，而不是几天或者几周。这样让每个人都做研究更高效。

机器学习发展会更好，更快。Jeff说机器学习社区发展得特别快。人们发布了一篇论文，一周内就有很多研究小组跟进，下载阅读，理解实现，再发布他们自己的扩展。这跟以前的计算机期刊投稿完全不同，等6个月才知道是否被接收，然后再过3个月最后发表。而现在把时间从一年压缩到一周，真不得了。

神经网络有很多固有的并行化，所有不同的神经元与其他的也是保持独立，特别本地接纳的，神经元仅仅接受一小部分比它更低的神经元作为输入。

优化的模型参数集不应该在一台机器上或者一台中心服务器上，应该有多个模型拷贝，这样协作区优化参数。

在训练过程中读取数据的随机部分。每一个拷贝在模型中获取当前的参数集，读取在当前梯度下的一点数据，找到想要的参数调整，在发送调整到中心的参数服务器中。这个参数服务器会对参数做调整。整个过程重复，这个也会在很多拷贝中进行。有些使用500份在500台不同机器上的拷贝，为了快速优化参数并处理大量数据。

一种方式是异步的，每一个都有自己的循环，取得参数，计算梯度，发送它们，不需要任何控制和跟其他的同步，不好的是当梯度返回到参数可能在计算结束后就被移走了。对有些例子可能有50到100份的拷贝。还有一种是同步，一个控制器控制所有的拷贝。

在过去的几年间，我们已经建立了两代用于训练和部署神经网络的计算机系统，并且将这些系统应用于解决很多在传统上来说对计算机而言很难的问题。我们对许多这些领域的最新技术做了很大的改进。

第一代系统DistBeliet在可扩缩性上表现很好，但在用于研究时灵活性达不到预期。对问题空间的更深理解让我们可以做出一些大幅度的简化。

这也是第二代系统的研发动机，用 TensorFlow 表达高层次的机器学习计算。它是C++语言编写的核心，冗余少。而不同的前端，现有Python和C++前端，添加其他语言的前端也不是难题。

我们输入数据、权重、误差以及标签，在不同节点进行不同的运算。

Tensor（张量）意味着N维数组。1维时就是向量，2维时就是矩阵；通过图像可以代表更高维的数据流，比如，图像可以用三维张量（行，列，颜色）来表示。

张量从图象的一端流动到另一端，这就是“TensorFlow”。“边”代表张量（数据），节点代表运算处理。

它能够在各个平台上自动运行模型：电话上，单个机器上（CPU或GPU），由成百上千的GPU卡组成的的分布式系统。

如果你还没想通过深度学习网络去解决你的数据问题，你还是要赶紧考虑。TensorFlow 让每个人更容易获取深度学习能力。

高度扩展的设计，更快的实验速度加速研究进程

容易分享模型，开发代码应用到可重用的效果

最后说一些quora上大家给Jeff Dean大神编的段子，供君一乐：

Jeff Dean当初面试Google时，被问到“如果P=NP能够推导出哪些结论”，Jeff回答说：“P = 0或者N = 1”。而在面试官还没笑完的时候，Jeff检查了一下Google的公钥，然后在黑板上写下了私钥。

编译器从不警告Jeff Dean，只有Jeff警告编译器。

Jeff Dean的编码速度在2000年底提高了约40倍，因为他换了USB2.0的键盘。

Jeff Dean曾经写过一个O(n2)算法，那是为了解决旅行商问题。

Jeff Dean的键盘只有两个键，1和0。

Jeff Dean失眠的时候，就Mapreduce羊。

转载请注明出处，关注如下我的微信公众号“董老师在硅谷”，关注硅谷趋势，一起学习成长。

