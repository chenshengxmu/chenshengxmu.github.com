---
layout: post
url: https://www.huxiu.com/article/14184
name: wugansha
time: 2013-05-09 14:16
title: 怎样才算大数据？（连载之一）
---
从谷歌趋势可以看到，大数据作为一个buzzword，是从2011年声名鹊起的。对这波趋势，中国跟进并不慢，旋即2012年被称作中国的大数据元年。其中两本书功不可没：前有涂子沛先生的《大数据》，从美国政府的数据信仰、政策和实践娓娓道来，让中国政坛和知识精英接受了一次思维洗礼，汪洋副总理离任广东前一系列开风气之先的大数据举措，当属此书之功；年末维克托.迈尔.舍恩伯格先生的《大数据时代》，则是系统论述大数据理念的奠基之作。如果说前者着力于发蒙—大数据可以做什么，后者则注重解惑—大数据该怎么做。

中国做事大气魄。原著为英文的《大数据时代》美国读者尚在翘首以盼，中文版在2012年末就摆上了国内读者的书架，原来是乘舍恩伯格先生参加云世界大会不失时机宣传。在年末年初的喧闹中，大数据产业园、大数据日、大数据专委会、大数据专业、大数据实验室和各种大数据峰会接踵而来。物联网和大数据、云计算和大数据彼此抱团取暖，来抵消决策者对层出不穷新概念的审美疲劳。其实，大数据还只是在民间热。相比起物联网和云计算，它在国家最高层面上获得的关注和实质支持还颇有不如，甚至美国政府都走在了前面：后者在2012年3月发布《大数据研究和发展倡议》，6个部门投资超过两亿美金推动相关研究。两亿美金对于工信部和科技部来说是毛毛雨，按兵不动是什么原因？

根据在下与工信部官员和智库的一些交流，我感觉决策者还存在很多疑惑：大数据究竟是什么新玩意儿？与以前的数据库、数据仓库、数据挖掘和商业智能有什么区别？市场有多大？中国应该重点发展什么？竞争优势和劣势在哪里？每每官员们在台上指点江山、大谈大数据战略云云（据在下目测，基本内容都来自2011年麦肯锡的《大数据：创新、竞争和生产力的下一个前沿》和2012年达沃斯的《大数据，大影响：全球发展的新可能》），台下一见专家就虚心请教大数据新在什么地方。

在下不揣浅陋，打算把对大数据的认识写下来，对大数据做一个深度的、非主流甚至是另类的解读。

当然从基本概念说起。大数据4个V：Volume（体量大），Velocity（快速化），Variety（类型杂），Value（价值大）。关于前3个V，很多人以讹传讹说是IBM首创的，其实是METAGroup（现为Gartner的一部分）的一个分析师Doug Laney早在2001年提出的（这位老兄专门写了一个博文吐槽他人冒功）。当然，IBM也不是全无贡献，它去掉了Value，加上Veracity（真实性），也算是自成一派。而其它公司只能暗恨字典里V字头的单词太少。

IDC对于每年创建和复制的信息之体量做了预测：2011年1.8ZB（ZB有多大，可以戳这里），2012年2.8ZB，按照每两年翻一番（摩尔定律是一切类似规律的滥觞）的速度，2020年达到40ZB。这个数据怎么算出来的？IDC秘而不宣。这个研究是在EMC赞助下的，EMC笑而不语。如果说对静态数据（data at rest）体量的预测有助于存储的销售，动态数据（data in motion）的体量无疑跟网络需求绑在一起。于是Cisco一个类似的预测说道：2016年数据移动的总量达到1.3ZB。其实所有这些数据加起来都不如谷歌Eric Schmidt的说法有感染力：从人类文明曙光到2003年数以万计的时间长河里人类一共产生了5EB（天知道他怎么算出来的），而到2010年每两天人类就能产生5EB的数据（这个有可能是从IDC的数据里推知的）。

这是不是业界巨擘们自我实现的预言？我觉得是。克里斯.安德森2008年在《连线》做了个专题“拍字节时代（The Petabyte Age）”，显然作为数字时代预言家的老安胆子不够大。

数据总量的增长主要归功于非结构化数据的增长。广义的非结构化数据也包括了半结构化和多结构化数据，目前普遍被认为占到总量的85%以上，而且增速比结构化数据快得多（有说法是快10-50倍）。低信息密度的非结构化数据是大数据的一大挑战，以后在Variety这一专题中会细细阐述。挑战就是机会，业界巨擘们创造了很多新的概念来迎接非结构化数据，NoSQL数据库是其中最亮丽的一个。对此，数据库界的老法师Mike Stonebraker对此耿耿于怀，不惜力推“血统”更纯正的NewSQL数据库；Sybase的CTO Irfan Khan甚至说大数据（这个新概念）根本就是个大谎言，声称他们的数据仓库工具早就能分析包括非结构化数据在内的大数据。

这类总量数据的预测，对于存储和网络企业的投资者来说，无疑能提升信心，但对其他人来说，没有太大意义。他们更关心的是个体行业、企业甚至个人数据的状况。

麦肯锡对大数据的定义就是从个体数据集的大体量入手的：大数据是指那些很大的数据集，大到传统的数据库软件工具已经无法采集、存储、管理和分析。传统数据库有效工作的数据大小一般来说在10-100TB，因此10-100TB通常成为大数据的门槛。无独有偶，IDC在给大数据做定义时也把阈值设在100TB（它同时也给出了velocity和variety的量化指标，以后再表）。其实这种方法未必科学，对于非结构化数据的存储来说，本来就跟数据库无关，而且传统文件系统能够处理的数据量往往受限于元数据而非原始数据大小，因此能处理的上限要比数据库要高。不管怎样，有一个简单明晰的数值来指导企业大数据的判断，总是好事。

